{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Outlier Exposure - Images.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-ZQIZmdw5Cy","executionInfo":{"status":"ok","timestamp":1653906944186,"user_tz":-300,"elapsed":3280,"user":{"displayName":"Mohammad Areeb Siddiqui","userId":"14081878860323978183"}},"outputId":"1c308fb7-0474-4c1c-e0b7-b61fdf715128"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coAxzcUddLPV","executionInfo":{"status":"ok","timestamp":1653906944187,"user_tz":-300,"elapsed":8,"user":{"displayName":"Mohammad Areeb Siddiqui","userId":"14081878860323978183"}},"outputId":"61faac6d-e3a7-42cd-f11a-d528a3a718b1"},"source":["cd '/content/drive/MyDrive/ClassAug Work/outlier-exposure'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/12PFZZk88jXqRuPPPvsCgycke0gd7aQBI/ClassAug Work/outlier-exposure\n"]}]},{"cell_type":"code","metadata":{"id":"uyHashdHc5tA"},"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import os\n","import pickle\n","import argparse\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","import torchvision.transforms as trn\n","import torchvision.datasets as dset\n","from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","if __package__ is None:\n","    import sys\n","    from os import path\n","\n","    from utils.validation_dataset import validation_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOaB5eKKeUgf"},"source":["batch_size=128\n","calibration=False\n","dataset='cifar10'\n","decay=0.0005\n","droprate=0.3\n","epochs=1000\n","layers=40\n","learning_rate=0.2\n","load=''\n","model='allconv'\n","momentum=0.9\n","ngpu=1\n","oe_batch_size=256\n","prefetch=0\n","save='./snapshots/oe_scratch_new'\n","test=False\n","test_bs=200\n","widen_factor=2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfVQ8DZ2quIg","executionInfo":{"status":"ok","timestamp":1653906945647,"user_tz":-300,"elapsed":9,"user":{"displayName":"Mohammad Areeb Siddiqui","userId":"14081878860323978183"}},"outputId":"be19f906-3ca6-4144-a9d8-e55800af1c7d"},"source":["print(torch.cuda.get_device_name(0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla T4\n"]}]},{"cell_type":"code","metadata":{"id":"tYiRuWLAlNYF"},"source":["class IDDataset(Dataset):\n","  def __init__(self, embs, labels):\n","    self.embs = torch.from_numpy(embs).float()\n","    self.labels = torch.from_numpy(np.squeeze(labels)).to(dtype=torch.long)\n","\n","  def __len__(self):\n","    return self.embs.shape[0]\n","\n","  def __getitem__(self, idx):\n","    return self.embs[idx], self.labels[idx]\n","\n","class OODDataset(Dataset):\n","  def __init__(self, embs):\n","    self.embs = torch.from_numpy(embs).float()\n","\n","  def __len__(self):\n","    return self.embs.shape[0]\n","\n","  def __getitem__(self, idx):\n","    return self.embs[idx], 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdoisrbCpdfX"},"source":["class OEModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super(OEModel, self).__init__()\n","\n","        self.l1 = nn.Linear(2048, 1024)\n","        self.classifier = nn.Linear(1024, num_classes)\n","        self.softmax = nn.Softmax()\n","\n","    def forward(self, x):\n","        x = self.l1(x)\n","        x = self.classifier(x)\n","        x = self.softmax(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"msstuQbOdAdP","executionInfo":{"status":"ok","timestamp":1653907566659,"user_tz":-300,"elapsed":621018,"user":{"displayName":"Mohammad Areeb Siddiqui","userId":"14081878860323978183"}},"outputId":"f6e70607-370c-4427-b7b7-d71f2b471b3a"},"source":["state = {'batch_size': 128, 'calibration': False, 'dataset': 'cifar10', 'decay': 0.0005, 'droprate': 0.3, 'epochs': 100, 'layers': 40, 'learning_rate': 0.1, 'load': '', 'model': 'allconv', 'momentum': 0.9, 'ngpu': 0, 'oe_batch_size': 256, 'prefetch': 0, 'save': './snapshots/oe_scratch_new', 'test': False, 'test_bs': 200, 'widen_factor': 2}\n","\n","torch.manual_seed(1)\n","np.random.seed(1)\n","\n","# mean and standard deviation of channels of CIFAR-10 images\n","mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n","std = [x / 255 for x in [63.0, 62.1, 66.7]]\n","\n","train_transform = trn.Compose([trn.RandomHorizontalFlip(), trn.RandomCrop(32, padding=4),\n","                               trn.ToTensor(), trn.Normalize(mean, std)])\n","test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n","\n","embs_data = None\n","with open('../embeddings/svhn_resnet50_embeddings.pkl', 'rb') as f:\n","    embs_data = pickle.load(f)\n","embs = embs_data['embs']\n","labels = embs_data['labels']\n","x_train, x_test, y_train, y_test = train_test_split(embs, labels, test_size=0.30, random_state=43)\n","train_data_in = IDDataset(x_train, y_train)\n","test_data = IDDataset(x_test, y_test)\n","num_classes = 10\n","\n","calib_indicator = ''\n","if calibration:\n","    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n","    calib_indicator = '_calib'\n","\n","embs_data = None\n","with open('../embeddings/tinyimagenet_resnet50_embeddings.pkl', 'rb') as f:\n","    embs_data = pickle.load(f)\n","ood_data = OODDataset(embs_data['embs'])\n","\n","train_loader_in = torch.utils.data.DataLoader(\n","    train_data_in,\n","    batch_size=batch_size, shuffle=True,\n","    num_workers=prefetch, pin_memory=True)\n","\n","train_loader_out = torch.utils.data.DataLoader(\n","    ood_data,\n","    batch_size=oe_batch_size, shuffle=False,\n","    num_workers=prefetch, pin_memory=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_data,\n","    batch_size=batch_size, shuffle=False,\n","    num_workers=prefetch, pin_memory=True)\n","\n","net = OEModel(num_classes)\n","\n","start_epoch = 0\n","# Restore model if desired\n","if load != '':\n","    for i in range(1000 - 1, -1, -1):\n","        model_name = os.path.join(load, dataset + calib_indicator + '_' + model +\n","                                  '_svhn_resnet50_oe_scratch_epoch_' + str(i) + '.pt')\n","        if os.path.isfile(model_name):\n","            net.load_state_dict(torch.load(model_name))\n","            print('Model restored! Epoch:', i)\n","            start_epoch = i + 1\n","            break\n","    if start_epoch == 0:\n","        assert False, \"could not resume\"\n","\n","if ngpu > 1:\n","    net = torch.nn.DataParallel(net, device_ids=list(range(ngpu)))\n","\n","if ngpu > 0:\n","    net.cuda()\n","    torch.cuda.manual_seed(1)\n","\n","cudnn.benchmark = True  # fire on all cylinders\n","\n","optimizer = torch.optim.SGD(net.parameters(), state['learning_rate'], momentum=state['momentum'], weight_decay=state['decay'], nesterov=True)\n","\n","def cosine_annealing(step, total_steps, lr_max, lr_min):\n","    return lr_min + (lr_max - lr_min) * 0.5 * (\n","            1 + np.cos(step / total_steps * np.pi))\n","\n","\n","scheduler = torch.optim.lr_scheduler.LambdaLR(\n","    optimizer,\n","    lr_lambda=lambda step: cosine_annealing(\n","        step,\n","        epochs * len(train_loader_in),\n","        1,  # since lr_lambda computes multiplicative factor\n","        1e-6 / learning_rate))\n","\n","\n","# /////////////// Training ///////////////\n","\n","def train():\n","    net.train()  # enter train mode\n","    loss_avg = 0.0\n","\n","    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n","    # train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n","    for in_set, out_set in zip(train_loader_in, train_loader_out):\n","        data = torch.cat((in_set[0], out_set[0]), 0)\n","        target = in_set[1]\n","\n","        data, target = data.cuda(), target.cuda()\n","\n","        # forward\n","        x = net(data)\n","\n","        # backward\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","        loss = F.cross_entropy(x[:len(in_set[0])], target)\n","        # cross-entropy from softmax distribution to uniform distribution\n","        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # exponential moving average\n","        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n","\n","    state['train_loss'] = loss_avg\n","\n","\n","# test function\n","def test():\n","    net.eval()\n","    loss_avg = 0.0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.cuda(), target.cuda()\n","\n","            # forward\n","            output = net(data)\n","            loss = F.cross_entropy(output, target)\n","\n","            # accuracy\n","            pred = output.data.max(1)[1]\n","            correct += pred.eq(target.data).sum().item()\n","\n","            # test loss average\n","            loss_avg += float(loss.data)\n","\n","    state['test_loss'] = loss_avg / len(test_loader)\n","    state['test_accuracy'] = correct / len(test_loader.dataset)\n","\n","if test:\n","    test()\n","    print(state)\n","    exit()\n","\n","# Make save directory\n","if not os.path.exists(save):\n","    os.makedirs(save)\n","if not os.path.isdir(save):\n","    raise Exception('%s is not a dir' % save)\n","\n","with open(os.path.join(save, dataset + calib_indicator + '_' + model +\n","                                  '_svhn_resnet50_oe_scratch_training_results.csv'), 'w') as f:\n","    f.write('epoch,time(s),train_loss,train_accuracy,test_loss,test_accuracy,test_error(%)\\n')\n","\n","print('Beginning Training\\n')\n","best_acc = 0\n","# Main loop\n","for epoch in range(start_epoch, epochs):\n","    state['epoch'] = epoch\n","\n","    begin_epoch = time.time()\n","\n","    train()\n","    test()\n","\n","    if 'test_accuracy' in state and best_acc < state['test_accuracy']:\n","      best_acc = state['test_accuracy']\n","      # Save model\n","      torch.save(net.state_dict(),\n","                os.path.join(save, dataset + calib_indicator + '_' + model +\n","                              '_svhn_resnet50_oe_scratch_epoch_' + str(epoch) + '-my.pt'))\n","      # Let us not waste space and delete the previous model\n","      prev_path = os.path.join(save, dataset + calib_indicator + '_' + model +\n","                              '_svhn_resnet50_oe_scratch_epoch_' + str(epoch - 1) + '-my.pt')\n","      if os.path.exists(prev_path): os.remove(prev_path)\n","\n","    # Show results\n","\n","    with open(os.path.join(save, dataset + calib_indicator + '_' + model +\n","                                      '_svhn_resnet50_oe_scratch_training_results.csv'), 'a') as f:\n","        f.write('%03d,%05d,%0.6f,%0.5f,%0.2f\\n' % (\n","            (epoch + 1),\n","            time.time() - begin_epoch,\n","            state['train_loss'],\n","            state['test_loss'],\n","            100 - 100. * state['test_accuracy'],\n","        ))\n","\n","    # # print state with rounded decimals\n","    # print('Epoch: ', epoch+1, ' | Time: ', int(time.time() - begin_epoch))\n","    # print({k: round(v, 4) if isinstance(v, float) else v for k, v in state.items()})\n","\n","    print(f'Epoch: {epoch + 1} \\| Time: {int(time.time() - begin_epoch)} \\| Train Loss: {state[\"train_loss\"]} \\| Test Loss: {state[\"test_loss\"]} \\| Test Accuracy: {state[\"test_accuracy\"]}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  if sys.path[0] == '':\n"]},{"output_type":"stream","name":"stdout","text":["{'batch_size': 128, 'calibration': False, 'dataset': 'cifar10', 'decay': 0.0005, 'droprate': 0.3, 'epochs': 100, 'layers': 40, 'learning_rate': 0.1, 'load': '', 'model': 'allconv', 'momentum': 0.9, 'ngpu': 0, 'oe_batch_size': 256, 'prefetch': 0, 'save': './snapshots/oe_scratch_new', 'test': False, 'test_bs': 200, 'widen_factor': 2, 'test_loss': 2.301273542412361, 'test_accuracy': 0.1070936985933461}\n","Beginning Training\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 \\| Time: 0 \\| Train Loss: 3.294176728673124 \\| Test Loss: 2.1266265641977857 \\| Test Accuracy: 0.32504112532312757\n","Epoch: 2 \\| Time: 0 \\| Train Loss: 3.2457052322618263 \\| Test Loss: 2.046775085731637 \\| Test Accuracy: 0.4198811562090845\n","Epoch: 3 \\| Time: 0 \\| Train Loss: 3.187594471133071 \\| Test Loss: 2.0419019859747825 \\| Test Accuracy: 0.4165911303588814\n","Epoch: 4 \\| Time: 0 \\| Train Loss: 3.201892193212589 \\| Test Loss: 2.0290356439582267 \\| Test Accuracy: 0.4267969248329808\n","Epoch: 5 \\| Time: 0 \\| Train Loss: 3.196080900918635 \\| Test Loss: 1.9960281736349343 \\| Test Accuracy: 0.4648000805720616\n","Epoch: 6 \\| Time: 0 \\| Train Loss: 3.164737217519896 \\| Test Loss: 1.9735126193500896 \\| Test Accuracy: 0.48675596736831506\n","Epoch: 7 \\| Time: 0 \\| Train Loss: 3.1549995737334737 \\| Test Loss: 1.978066052490038 \\| Test Accuracy: 0.4800752005908618\n","Epoch: 8 \\| Time: 0 \\| Train Loss: 3.118819509126772 \\| Test Loss: 1.9713370191181165 \\| Test Accuracy: 0.486688823983617\n","Epoch: 9 \\| Time: 0 \\| Train Loss: 3.130886756336509 \\| Test Loss: 1.9539372005176134 \\| Test Accuracy: 0.5048846812367811\n","Epoch: 10 \\| Time: 0 \\| Train Loss: 3.1001393313784478 \\| Test Loss: 1.9329536585337104 \\| Test Accuracy: 0.5298955920367946\n","Epoch: 11 \\| Time: 0 \\| Train Loss: 3.098468393542859 \\| Test Loss: 1.9284339657157277 \\| Test Accuracy: 0.53412562527277\n","Epoch: 12 \\| Time: 0 \\| Train Loss: 3.1209559115380467 \\| Test Loss: 1.940337145277359 \\| Test Accuracy: 0.5175076375600094\n","Epoch: 13 \\| Time: 0 \\| Train Loss: 3.0865437660383783 \\| Test Loss: 1.9034079957929293 \\| Test Accuracy: 0.5589015342263404\n","Epoch: 14 \\| Time: 0 \\| Train Loss: 3.0812391833086874 \\| Test Loss: 1.9458674269172767 \\| Test Accuracy: 0.5115990197065834\n","Epoch: 15 \\| Time: 0 \\| Train Loss: 3.0873763980483404 \\| Test Loss: 1.927679893796536 \\| Test Accuracy: 0.5308691711149159\n","Epoch: 16 \\| Time: 0 \\| Train Loss: 3.07785343982127 \\| Test Loss: 1.8924484892464504 \\| Test Accuracy: 0.5686037533152046\n","Epoch: 17 \\| Time: 0 \\| Train Loss: 3.078719574670342 \\| Test Loss: 1.8932250681864857 \\| Test Accuracy: 0.5660858763890287\n","Epoch: 18 \\| Time: 0 \\| Train Loss: 3.0677931136711427 \\| Test Loss: 1.880983068196047 \\| Test Accuracy: 0.5806559908684997\n","Epoch: 19 \\| Time: 0 \\| Train Loss: 3.061584054952786 \\| Test Loss: 1.8802463327866255 \\| Test Accuracy: 0.5805552757914526\n","Epoch: 20 \\| Time: 0 \\| Train Loss: 3.0532214436170912 \\| Test Loss: 1.8652849391806279 \\| Test Accuracy: 0.5967032598113271\n","Epoch: 21 \\| Time: 0 \\| Train Loss: 3.057194980699192 \\| Test Loss: 1.8614263928499344 \\| Test Accuracy: 0.5987847047369658\n","Epoch: 22 \\| Time: 0 \\| Train Loss: 3.03984365481923 \\| Test Loss: 1.8868390193824605 \\| Test Accuracy: 0.5741095108604425\n","Epoch: 23 \\| Time: 0 \\| Train Loss: 3.052736618995679 \\| Test Loss: 1.967458395487249 \\| Test Accuracy: 0.4887031255245577\n","Epoch: 24 \\| Time: 0 \\| Train Loss: 3.0263996274555276 \\| Test Loss: 1.894049083214461 \\| Test Accuracy: 0.5640044314633901\n","Epoch: 25 \\| Time: 0 \\| Train Loss: 3.0573000104328187 \\| Test Loss: 1.8957280805694188 \\| Test Accuracy: 0.5620572733071474\n","Epoch: 26 \\| Time: 0 \\| Train Loss: 3.071803394096061 \\| Test Loss: 1.8661907440603036 \\| Test Accuracy: 0.5938160942693121\n","Epoch: 27 \\| Time: 0 \\| Train Loss: 3.0346623286038303 \\| Test Loss: 1.8674204006727162 \\| Test Accuracy: 0.5927418001141438\n","Epoch: 28 \\| Time: 0 \\| Train Loss: 3.0366785776973035 \\| Test Loss: 1.86448934968449 \\| Test Accuracy: 0.5969718333501192\n","Epoch: 29 \\| Time: 0 \\| Train Loss: 3.0218754016919314 \\| Test Loss: 1.853868371427315 \\| Test Accuracy: 0.60848692382583\n","Epoch: 30 \\| Time: 0 \\| Train Loss: 3.039222409671999 \\| Test Loss: 1.8810042168449435 \\| Test Accuracy: 0.5796488400980293\n","Epoch: 31 \\| Time: 0 \\| Train Loss: 3.043704204604753 \\| Test Loss: 1.870069506342319 \\| Test Accuracy: 0.5898210628797798\n","Epoch: 32 \\| Time: 0 \\| Train Loss: 3.0226083174732783 \\| Test Loss: 1.8936209218184836 \\| Test Accuracy: 0.5632994259240608\n","Epoch: 33 \\| Time: 0 \\| Train Loss: 3.0278538854977266 \\| Test Loss: 1.8632056641476349 \\| Test Accuracy: 0.5980796991976366\n","Epoch: 34 \\| Time: 0 \\| Train Loss: 3.0327199680482915 \\| Test Loss: 1.8970385452196834 \\| Test Accuracy: 0.5614194111525161\n","Epoch: 35 \\| Time: 0 \\| Train Loss: 3.106628196732717 \\| Test Loss: 1.8783431140138356 \\| Test Accuracy: 0.5807902776378957\n","Epoch: 36 \\| Time: 0 \\| Train Loss: 3.0373859853094975 \\| Test Loss: 1.8584103425684917 \\| Test Accuracy: 0.6012690099707926\n","Epoch: 37 \\| Time: 0 \\| Train Loss: 3.050020090137629 \\| Test Loss: 1.878246701326493 \\| Test Accuracy: 0.581595998254272\n","Epoch: 38 \\| Time: 0 \\| Train Loss: 3.045230632428261 \\| Test Loss: 1.8446524547404997 \\| Test Accuracy: 0.6158055527579145\n","Epoch: 39 \\| Time: 0 \\| Train Loss: 3.0358332566406094 \\| Test Loss: 1.8766781720992323 \\| Test Accuracy: 0.5839124450263538\n","Epoch: 40 \\| Time: 0 \\| Train Loss: 3.0284295178842644 \\| Test Loss: 1.8710749164671345 \\| Test Accuracy: 0.5886124819552153\n","Epoch: 41 \\| Time: 0 \\| Train Loss: 3.004474409771708 \\| Test Loss: 1.8636462575887918 \\| Test Accuracy: 0.596501829657233\n","Epoch: 42 \\| Time: 0 \\| Train Loss: 3.0613846602925703 \\| Test Loss: 1.8630673174182744 \\| Test Accuracy: 0.596468257964884\n","Epoch: 43 \\| Time: 0 \\| Train Loss: 3.0292197015210074 \\| Test Loss: 1.855222709700785 \\| Test Accuracy: 0.6051297545909289\n","Epoch: 44 \\| Time: 0 \\| Train Loss: 3.0046440774782868 \\| Test Loss: 1.8382358765909088 \\| Test Accuracy: 0.6251720549232886\n","Epoch: 45 \\| Time: 0 \\| Train Loss: 3.030645868521526 \\| Test Loss: 1.882002727156545 \\| Test Accuracy: 0.5778695404035318\n","Epoch: 46 \\| Time: 0 \\| Train Loss: 3.0329722564968065 \\| Test Loss: 1.8342938668738107 \\| Test Accuracy: 0.6265149226172492\n","Epoch: 47 \\| Time: 0 \\| Train Loss: 3.0150764086354 \\| Test Loss: 1.8743804949035971 \\| Test Accuracy: 0.5832410111793735\n","Epoch: 48 \\| Time: 0 \\| Train Loss: 3.016459766598714 \\| Test Loss: 1.8592201000631112 \\| Test Accuracy: 0.6019740155101219\n","Epoch: 49 \\| Time: 0 \\| Train Loss: 3.0233739022310884 \\| Test Loss: 1.8436882946112636 \\| Test Accuracy: 0.6162755564508007\n","Epoch: 50 \\| Time: 0 \\| Train Loss: 3.04434079619635 \\| Test Loss: 1.8499359869649994 \\| Test Accuracy: 0.6119112364454292\n","Epoch: 51 \\| Time: 0 \\| Train Loss: 3.025222208442011 \\| Test Loss: 1.8533918719434943 \\| Test Accuracy: 0.6062711921307953\n","Epoch: 52 \\| Time: 0 \\| Train Loss: 3.014082165871028 \\| Test Loss: 1.8693595346974712 \\| Test Accuracy: 0.5904253533420619\n","Epoch: 53 \\| Time: 0 \\| Train Loss: 3.0119321789666675 \\| Test Loss: 1.873466373513185 \\| Test Accuracy: 0.5866653237989727\n","Epoch: 54 \\| Time: 0 \\| Train Loss: 3.0124558940657744 \\| Test Loss: 1.8412523331048662 \\| Test Accuracy: 0.62003558599389\n","Epoch: 55 \\| Time: 0 \\| Train Loss: 3.0133559222788064 \\| Test Loss: 1.8336507005241296 \\| Test Accuracy: 0.6272870715412764\n","Epoch: 56 \\| Time: 0 \\| Train Loss: 3.0285664514879076 \\| Test Loss: 1.8324629365118787 \\| Test Accuracy: 0.629267801389868\n","Epoch: 57 \\| Time: 0 \\| Train Loss: 3.028304459977974 \\| Test Loss: 1.851864540525772 \\| Test Accuracy: 0.6079833484405949\n","Epoch: 58 \\| Time: 0 \\| Train Loss: 3.0002685836813052 \\| Test Loss: 1.856932728587302 \\| Test Accuracy: 0.6026118776647531\n","Epoch: 59 \\| Time: 0 \\| Train Loss: 3.0093509788375075 \\| Test Loss: 1.8512958272843913 \\| Test Accuracy: 0.6101319367509316\n","Epoch: 60 \\| Time: 0 \\| Train Loss: 3.012895510402605 \\| Test Loss: 1.8553411065253065 \\| Test Accuracy: 0.6043576056669017\n","Epoch: 61 \\| Time: 0 \\| Train Loss: 3.0240974142514023 \\| Test Loss: 1.8637231526968305 \\| Test Accuracy: 0.5954946788867627\n","Epoch: 62 \\| Time: 0 \\| Train Loss: 2.9942493847445495 \\| Test Loss: 1.851318746677284 \\| Test Accuracy: 0.6084533521334811\n","Epoch: 63 \\| Time: 0 \\| Train Loss: 3.0199979683922367 \\| Test Loss: 1.8432907193515433 \\| Test Accuracy: 0.6177862826065061\n","Epoch: 64 \\| Time: 0 \\| Train Loss: 2.995571646477625 \\| Test Loss: 1.8407026754428388 \\| Test Accuracy: 0.6207741632255682\n","Epoch: 65 \\| Time: 0 \\| Train Loss: 2.990218321526075 \\| Test Loss: 1.8324636400001755 \\| Test Accuracy: 0.629368516466915\n","Epoch: 66 \\| Time: 0 \\| Train Loss: 3.001349871198429 \\| Test Loss: 1.8531241509024166 \\| Test Accuracy: 0.6076476315171048\n","Epoch: 67 \\| Time: 0 \\| Train Loss: 3.0185700680145096 \\| Test Loss: 1.8263839815819212 \\| Test Accuracy: 0.6353442777050391\n","Epoch: 68 \\| Time: 0 \\| Train Loss: 3.000991352456735 \\| Test Loss: 1.8397041779219336 \\| Test Accuracy: 0.6205391613791251\n","Epoch: 69 \\| Time: 0 \\| Train Loss: 3.0096274935961653 \\| Test Loss: 1.834738725756371 \\| Test Accuracy: 0.6274885016953705\n","Epoch: 70 \\| Time: 0 \\| Train Loss: 3.0168870969703683 \\| Test Loss: 1.839700941364141 \\| Test Accuracy: 0.6207070198408702\n","Epoch: 71 \\| Time: 0 \\| Train Loss: 3.0165808985947993 \\| Test Loss: 1.8130621152885993 \\| Test Accuracy: 0.6513244032631685\n","Epoch: 72 \\| Time: 0 \\| Train Loss: 3.039501910318525 \\| Test Loss: 1.8565635133710543 \\| Test Accuracy: 0.6030147379729413\n","Epoch: 73 \\| Time: 0 \\| Train Loss: 3.0476853108173705 \\| Test Loss: 1.8553742850798907 \\| Test Accuracy: 0.6058683318226071\n","Epoch: 74 \\| Time: 0 \\| Train Loss: 2.9826905116485705 \\| Test Loss: 1.852087929525089 \\| Test Accuracy: 0.6073790579783127\n","Epoch: 75 \\| Time: 0 \\| Train Loss: 3.026265931460378 \\| Test Loss: 1.8513927730879558 \\| Test Accuracy: 0.6085876389028771\n","Epoch: 76 \\| Time: 0 \\| Train Loss: 3.0318115912536348 \\| Test Loss: 1.841546048933856 \\| Test Accuracy: 0.6188941484540236\n","Epoch: 77 \\| Time: 0 \\| Train Loss: 2.994322122949698 \\| Test Loss: 1.8206012750388214 \\| Test Accuracy: 0.6417228992513513\n","Epoch: 78 \\| Time: 0 \\| Train Loss: 3.023325535713108 \\| Test Loss: 1.8368644259006681 \\| Test Accuracy: 0.6234263269211401\n","Epoch: 79 \\| Time: 0 \\| Train Loss: 3.0009176523190977 \\| Test Loss: 1.8472113026058214 \\| Test Accuracy: 0.6133548192164368\n","Epoch: 80 \\| Time: 0 \\| Train Loss: 3.024065751585565 \\| Test Loss: 1.8831875774481777 \\| Test Accuracy: 0.5738409373216504\n","Epoch: 81 \\| Time: 0 \\| Train Loss: 3.0100971638349794 \\| Test Loss: 1.8590815358919135 \\| Test Accuracy: 0.599489710276295\n","Epoch: 82 \\| Time: 0 \\| Train Loss: 2.990752149983066 \\| Test Loss: 1.8311751589754621 \\| Test Accuracy: 0.631953536777789\n","Epoch: 83 \\| Time: 0 \\| Train Loss: 3.0017727952659023 \\| Test Loss: 1.839107625474234 \\| Test Accuracy: 0.6199684426091919\n","Epoch: 84 \\| Time: 0 \\| Train Loss: 3.0021043604697875 \\| Test Loss: 1.8221368400835685 \\| Test Accuracy: 0.6403464598650418\n","Epoch: 85 \\| Time: 0 \\| Train Loss: 2.9758607499593515 \\| Test Loss: 1.8381164769757972 \\| Test Accuracy: 0.6225198912277168\n","Epoch: 86 \\| Time: 0 \\| Train Loss: 3.0134854454768845 \\| Test Loss: 1.829347004194628 \\| Test Accuracy: 0.6326249706247692\n","Epoch: 87 \\| Time: 0 \\| Train Loss: 3.005914656196535 \\| Test Loss: 1.8345990237248302 \\| Test Accuracy: 0.6256756303085238\n","Epoch: 88 \\| Time: 0 \\| Train Loss: 3.0096291157237722 \\| Test Loss: 1.8524497599049188 \\| Test Accuracy: 0.6074797730553597\n","Epoch: 89 \\| Time: 0 \\| Train Loss: 3.0042442493550285 \\| Test Loss: 1.835825706756166 \\| Test Accuracy: 0.6246349078457045\n","Epoch: 90 \\| Time: 0 \\| Train Loss: 3.0077022897259225 \\| Test Loss: 1.872874998227721 \\| Test Accuracy: 0.5862288917984355\n","Epoch: 91 \\| Time: 0 \\| Train Loss: 3.0091963021271964 \\| Test Loss: 1.8479682621526103 \\| Test Accuracy: 0.6139926813710679\n","Epoch: 92 \\| Time: 0 \\| Train Loss: 3.021657828089295 \\| Test Loss: 1.8318400838344393 \\| Test Accuracy: 0.6298385201598012\n","Epoch: 93 \\| Time: 0 \\| Train Loss: 3.002116262935024 \\| Test Loss: 1.837612893959995 \\| Test Accuracy: 0.623057038305301\n","Epoch: 94 \\| Time: 0 \\| Train Loss: 2.9981259655440695 \\| Test Loss: 1.8553729379637558 \\| Test Accuracy: 0.6042904622822036\n","Epoch: 95 \\| Time: 0 \\| Train Loss: 3.0017935415337447 \\| Test Loss: 1.8279529180649523 \\| Test Accuracy: 0.6326249706247692\n","Epoch: 96 \\| Time: 0 \\| Train Loss: 3.0209115016560855 \\| Test Loss: 1.8315566921438782 \\| Test Accuracy: 0.629267801389868\n","Epoch: 97 \\| Time: 0 \\| Train Loss: 3.0458778755829896 \\| Test Loss: 1.9082949238273719 \\| Test Accuracy: 0.5492328868298251\n","Epoch: 98 \\| Time: 0 \\| Train Loss: 2.9964283575729502 \\| Test Loss: 1.8293972199566886 \\| Test Accuracy: 0.6329271158559103\n","Epoch: 99 \\| Time: 0 \\| Train Loss: 3.0022071821757152 \\| Test Loss: 1.8863209496240247 \\| Test Accuracy: 0.5718266357807097\n","Epoch: 100 \\| Time: 0 \\| Train Loss: 3.0177119348398866 \\| Test Loss: 1.83601149827114 \\| Test Accuracy: 0.6247020512304026\n","Epoch: 101 \\| Time: 0 \\| Train Loss: 2.993071051296858 \\| Test Loss: 1.852885794741913 \\| Test Accuracy: 0.6073790579783127\n","Epoch: 102 \\| Time: 0 \\| Train Loss: 3.026751765064926 \\| Test Loss: 1.840711972744168 \\| Test Accuracy: 0.6200020143015409\n","Epoch: 103 \\| Time: 0 \\| Train Loss: 2.9820294970175842 \\| Test Loss: 1.8352451779811678 \\| Test Accuracy: 0.6251720549232886\n","Epoch: 104 \\| Time: 0 \\| Train Loss: 3.0210172904659958 \\| Test Loss: 1.8246664811613222 \\| Test Accuracy: 0.6374592943230268\n","Epoch: 105 \\| Time: 0 \\| Train Loss: 3.0657156187878836 \\| Test Loss: 1.9626219901915785 \\| Test Accuracy: 0.4940410246080505\n","Epoch: 106 \\| Time: 0 \\| Train Loss: 3.039737944265098 \\| Test Loss: 1.8755295015711642 \\| Test Accuracy: 0.5825360056400443\n","Epoch: 107 \\| Time: 0 \\| Train Loss: 3.0264283485666086 \\| Test Loss: 1.8615928540413984 \\| Test Accuracy: 0.5959982542719978\n","Epoch: 108 \\| Time: 0 \\| Train Loss: 3.0157987395978467 \\| Test Loss: 1.8518935656854523 \\| Test Accuracy: 0.6078154899788498\n","Epoch: 109 \\| Time: 0 \\| Train Loss: 3.010129456636851 \\| Test Loss: 1.835772596715346 \\| Test Accuracy: 0.6242656192298653\n","Epoch: 110 \\| Time: 0 \\| Train Loss: 3.0054669285321016 \\| Test Loss: 1.839526112499155 \\| Test Accuracy: 0.6211770235337564\n","Epoch: 111 \\| Time: 0 \\| Train Loss: 2.996608627783164 \\| Test Loss: 1.838944421305677 \\| Test Accuracy: 0.6225534629200657\n","Epoch: 112 \\| Time: 0 \\| Train Loss: 2.986689606328482 \\| Test Loss: 1.837281128367641 \\| Test Accuracy: 0.6224863195353678\n","Epoch: 113 \\| Time: 0 \\| Train Loss: 3.0047676483504304 \\| Test Loss: 1.8530810718372657 \\| Test Accuracy: 0.6072111995165677\n","Epoch: 114 \\| Time: 0 \\| Train Loss: 3.0384443682935554 \\| Test Loss: 1.8755245413391375 \\| Test Accuracy: 0.5828717225635345\n","Epoch: 115 \\| Time: 0 \\| Train Loss: 3.0165048603923426 \\| Test Loss: 1.8724946776173146 \\| Test Accuracy: 0.587706046261792\n","Epoch: 116 \\| Time: 0 \\| Train Loss: 2.997446498011342 \\| Test Loss: 1.8249812090345718 \\| Test Accuracy: 0.6377950112465169\n","Epoch: 117 \\| Time: 0 \\| Train Loss: 3.0123437571138325 \\| Test Loss: 1.8293521486126814 \\| Test Accuracy: 0.6329606875482593\n","Epoch: 118 \\| Time: 0 \\| Train Loss: 3.000559493680363 \\| Test Loss: 1.8501740392185588 \\| Test Accuracy: 0.6087219256722731\n","Epoch: 119 \\| Time: 0 \\| Train Loss: 3.027250819826181 \\| Test Loss: 1.8591556850932698 \\| Test Accuracy: 0.5989525631987108\n","Epoch: 120 \\| Time: 0 \\| Train Loss: 3.003817639632063 \\| Test Loss: 1.8835700581513761 \\| Test Accuracy: 0.575620237016148\n","Epoch: 121 \\| Time: 0 \\| Train Loss: 3.011847740055723 \\| Test Loss: 1.8177063930699753 \\| Test Accuracy: 0.643938630946386\n","Epoch: 122 \\| Time: 0 \\| Train Loss: 3.0232583950873284 \\| Test Loss: 1.8216635602738214 \\| Test Accuracy: 0.6394400241716185\n","Epoch: 123 \\| Time: 0 \\| Train Loss: 3.001766711133687 \\| Test Loss: 1.836391436183913 \\| Test Accuracy: 0.6241313324604694\n","Epoch: 124 \\| Time: 0 \\| Train Loss: 3.0281636095416435 \\| Test Loss: 1.853325706183143 \\| Test Accuracy: 0.6075804881324067\n","Epoch: 125 \\| Time: 0 \\| Train Loss: 3.027061413286777 \\| Test Loss: 1.8609341086236193 \\| Test Accuracy: 0.5987847047369658\n","Epoch: 126 \\| Time: 0 \\| Train Loss: 3.0206774773353238 \\| Test Loss: 1.8464559521286272 \\| Test Accuracy: 0.6137241078322758\n","Epoch: 127 \\| Time: 0 \\| Train Loss: 3.0051814674617274 \\| Test Loss: 1.845937541626042 \\| Test Accuracy: 0.6144291133716051\n","Epoch: 128 \\| Time: 0 \\| Train Loss: 2.988969528896755 \\| Test Loss: 1.8314111872292385 \\| Test Accuracy: 0.6299392352368482\n","Epoch: 129 \\| Time: 0 \\| Train Loss: 2.97306382611705 \\| Test Loss: 1.8257330461633052 \\| Test Accuracy: 0.6362842850908115\n","Epoch: 130 \\| Time: 0 \\| Train Loss: 3.0024990634548456 \\| Test Loss: 1.8368567724596276 \\| Test Accuracy: 0.6244670493839595\n","Epoch: 131 \\| Time: 0 \\| Train Loss: 3.0108812307352286 \\| Test Loss: 1.8223220933660418 \\| Test Accuracy: 0.6394400241716185\n","Epoch: 132 \\| Time: 0 \\| Train Loss: 2.9973330587703306 \\| Test Loss: 1.835433383867976 \\| Test Accuracy: 0.6247691946151005\n","Epoch: 133 \\| Time: 0 \\| Train Loss: 2.982843471727782 \\| Test Loss: 1.866133693461766 \\| Test Accuracy: 0.5937825225769631\n","Epoch: 134 \\| Time: 0 \\| Train Loss: 3.008290868823413 \\| Test Loss: 1.8269027926891146 \\| Test Accuracy: 0.6347399872427569\n","Epoch: 135 \\| Time: 0 \\| Train Loss: 2.9878380429286535 \\| Test Loss: 1.8395451430087437 \\| Test Accuracy: 0.6222848893812737\n","Epoch: 136 \\| Time: 0 \\| Train Loss: 3.0363490830192417 \\| Test Loss: 1.8703616238458984 \\| Test Accuracy: 0.5890824856481015\n","Epoch: 137 \\| Time: 0 \\| Train Loss: 2.986711977084491 \\| Test Loss: 1.8188931215474533 \\| Test Accuracy: 0.6423943330983315\n","Epoch: 138 \\| Time: 0 \\| Train Loss: 2.9969342003329733 \\| Test Loss: 1.8555890526382708 \\| Test Accuracy: 0.605364756437372\n","Epoch: 139 \\| Time: 0 \\| Train Loss: 3.004821366308306 \\| Test Loss: 1.8439876654629033 \\| Test Accuracy: 0.6177527109141572\n","Epoch: 140 \\| Time: 0 \\| Train Loss: 3.0441096937883323 \\| Test Loss: 1.8468630743640686 \\| Test Accuracy: 0.6122469533689193\n","Epoch: 141 \\| Time: 0 \\| Train Loss: 3.004046424100477 \\| Test Loss: 1.858978105716951 \\| Test Accuracy: 0.6005975761238124\n","Epoch: 142 \\| Time: 0 \\| Train Loss: 2.996710171458774 \\| Test Loss: 1.8489851281366634 \\| Test Accuracy: 0.6112062309060999\n","Epoch: 143 \\| Time: 0 \\| Train Loss: 3.0202007162455806 \\| Test Loss: 1.8652474578358074 \\| Test Accuracy: 0.5956961090408568\n","Epoch: 144 \\| Time: 0 \\| Train Loss: 2.994295633622756 \\| Test Loss: 1.8384714725191502 \\| Test Accuracy: 0.623057038305301\n","Epoch: 145 \\| Time: 0 \\| Train Loss: 3.0066210177678956 \\| Test Loss: 1.8157826262993875 \\| Test Accuracy: 0.6458186457179307\n","Epoch: 146 \\| Time: 0 \\| Train Loss: 2.9778714108124267 \\| Test Loss: 1.825366554853742 \\| Test Accuracy: 0.6375935810924229\n","Epoch: 147 \\| Time: 0 \\| Train Loss: 3.0113348126035566 \\| Test Loss: 1.837373934078626 \\| Test Accuracy: 0.6224863195353678\n","Epoch: 148 \\| Time: 0 \\| Train Loss: 3.012358217437885 \\| Test Loss: 1.8997647255787011 \\| Test Accuracy: 0.5599422566891596\n","Epoch: 149 \\| Time: 0 \\| Train Loss: 2.996627504035793 \\| Test Loss: 1.8522367165324003 \\| Test Accuracy: 0.6063047638231444\n","Epoch: 150 \\| Time: 0 \\| Train Loss: 3.0122876112323977 \\| Test Loss: 1.830038925097224 \\| Test Accuracy: 0.6314499613925538\n","Epoch: 151 \\| Time: 0 \\| Train Loss: 3.0283788931946223 \\| Test Loss: 1.8674614654590131 \\| Test Accuracy: 0.5927418001141438\n","Epoch: 152 \\| Time: 0 \\| Train Loss: 2.990948592365503 \\| Test Loss: 1.845612447149252 \\| Test Accuracy: 0.6157719810655655\n","Epoch: 153 \\| Time: 0 \\| Train Loss: 3.010982222412304 \\| Test Loss: 1.8559010990699474 \\| Test Accuracy: 0.6043576056669017\n","Epoch: 154 \\| Time: 0 \\| Train Loss: 3.0005698362391144 \\| Test Loss: 1.8527460522917718 \\| Test Accuracy: 0.6070433410548226\n","Epoch: 155 \\| Time: 0 \\| Train Loss: 3.030430696161591 \\| Test Loss: 1.8532079069399527 \\| Test Accuracy: 0.6074797730553597\n","Epoch: 156 \\| Time: 0 \\| Train Loss: 2.9883120834944688 \\| Test Loss: 1.8268617719028128 \\| Test Accuracy: 0.6358142813979253\n","Epoch: 157 \\| Time: 0 \\| Train Loss: 2.999138349857399 \\| Test Loss: 1.830120373181519 \\| Test Accuracy: 0.6315842481619498\n","Epoch: 158 \\| Time: 0 \\| Train Loss: 2.9965002178113154 \\| Test Loss: 1.854057490569839 \\| Test Accuracy: 0.6049283244368349\n","Epoch: 159 \\| Time: 0 \\| Train Loss: 3.005660489587725 \\| Test Loss: 1.8436172310374837 \\| Test Accuracy: 0.6174841373753651\n","Epoch: 160 \\| Time: 0 \\| Train Loss: 2.997284229823158 \\| Test Loss: 1.8208211419920042 \\| Test Accuracy: 0.6413200389431631\n","Epoch: 161 \\| Time: 0 \\| Train Loss: 3.029065942024615 \\| Test Loss: 1.8328648370734613 \\| Test Accuracy: 0.629032799543425\n","Epoch: 162 \\| Time: 0 \\| Train Loss: 3.0300010787632816 \\| Test Loss: 1.892248639732983 \\| Test Accuracy: 0.5676973176217813\n","Epoch: 163 \\| Time: 0 \\| Train Loss: 2.9876342248423247 \\| Test Loss: 1.84139634355455 \\| Test Accuracy: 0.6184912881458354\n","Epoch: 164 \\| Time: 0 \\| Train Loss: 3.0019337013871823 \\| Test Loss: 1.8273749719873518 \\| Test Accuracy: 0.6348742740121529\n","Epoch: 165 \\| Time: 0 \\| Train Loss: 2.997722647585138 \\| Test Loss: 1.8314954763829965 \\| Test Accuracy: 0.6299056635444993\n","Epoch: 166 \\| Time: 0 \\| Train Loss: 3.023314636988721 \\| Test Loss: 1.867885037041529 \\| Test Accuracy: 0.5913317890354853\n","Epoch: 167 \\| Time: 0 \\| Train Loss: 2.991725493974367 \\| Test Loss: 1.8204622734258102 \\| Test Accuracy: 0.6430321952529627\n","Epoch: 168 \\| Time: 0 \\| Train Loss: 3.002310199892318 \\| Test Loss: 1.8349172301558465 \\| Test Accuracy: 0.6252056266156377\n","Epoch: 169 \\| Time: 0 \\| Train Loss: 2.977604357206779 \\| Test Loss: 1.8522587134602755 \\| Test Accuracy: 0.6067076241313325\n","Epoch: 170 \\| Time: 0 \\| Train Loss: 2.986080007501679 \\| Test Loss: 1.8278498961690157 \\| Test Accuracy: 0.6329606875482593\n","Epoch: 171 \\| Time: 0 \\| Train Loss: 2.9714396494954287 \\| Test Loss: 1.8127907990386045 \\| Test Accuracy: 0.649947963876859\n","Epoch: 172 \\| Time: 0 \\| Train Loss: 2.9884566501811842 \\| Test Loss: 1.8380336014497944 \\| Test Accuracy: 0.6224527478430187\n","Epoch: 173 \\| Time: 0 \\| Train Loss: 3.0443463140179947 \\| Test Loss: 1.9835415705079174 \\| Test Accuracy: 0.4724879981199852\n","Epoch: 174 \\| Time: 0 \\| Train Loss: 3.008830185425665 \\| Test Loss: 1.8208253347822525 \\| Test Accuracy: 0.6402121730956457\n","Epoch: 175 \\| Time: 0 \\| Train Loss: 3.042445320443569 \\| Test Loss: 1.8601551178698887 \\| Test Accuracy: 0.5979454124282405\n","Epoch: 176 \\| Time: 0 \\| Train Loss: 2.995586736299581 \\| Test Loss: 1.8187842420242375 \\| Test Accuracy: 0.6428979084835666\n","Epoch: 177 \\| Time: 0 \\| Train Loss: 3.0263134363687705 \\| Test Loss: 1.8484680903316055 \\| Test Accuracy: 0.6118776647530803\n","Epoch: 178 \\| Time: 0 \\| Train Loss: 2.997542355247208 \\| Test Loss: 1.8493527327484327 \\| Test Accuracy: 0.6104005102897238\n","Epoch: 179 \\| Time: 0 \\| Train Loss: 2.9927148759956608 \\| Test Loss: 1.8423028309457803 \\| Test Accuracy: 0.6177191392218082\n","Epoch: 180 \\| Time: 0 \\| Train Loss: 3.0221409184832075 \\| Test Loss: 1.8506634393986714 \\| Test Accuracy: 0.6086547822875751\n","Epoch: 181 \\| Time: 0 \\| Train Loss: 3.0049737780034995 \\| Test Loss: 1.8335212402589331 \\| Test Accuracy: 0.6263470641555041\n","Epoch: 182 \\| Time: 0 \\| Train Loss: 3.009923856663835 \\| Test Loss: 1.831201949344684 \\| Test Accuracy: 0.6302078087756404\n","Epoch: 183 \\| Time: 0 \\| Train Loss: 3.034259155224171 \\| Test Loss: 1.8612850096092715 \\| Test Accuracy: 0.5970389767348172\n","Epoch: 184 \\| Time: 0 \\| Train Loss: 2.9990717302028838 \\| Test Loss: 1.8386928119372912 \\| Test Accuracy: 0.6218148856883875\n","Epoch: 185 \\| Time: 0 \\| Train Loss: 3.012930285926372 \\| Test Loss: 1.8354473390292712 \\| Test Accuracy: 0.6278577903112096\n","Epoch: 186 \\| Time: 0 \\| Train Loss: 3.026691176354338 \\| Test Loss: 1.8387262601197534 \\| Test Accuracy: 0.6217813139960385\n","Epoch: 187 \\| Time: 0 \\| Train Loss: 3.0029348098483677 \\| Test Loss: 1.8249114644373947 \\| Test Accuracy: 0.637895726323564\n","Epoch: 188 \\| Time: 0 \\| Train Loss: 2.9742973551117653 \\| Test Loss: 1.8519014381032133 \\| Test Accuracy: 0.6076476315171048\n","Epoch: 189 \\| Time: 0 \\| Train Loss: 2.989543533167403 \\| Test Loss: 1.8210259461095917 \\| Test Accuracy: 0.6403464598650418\n","Epoch: 190 \\| Time: 0 \\| Train Loss: 3.019709250594367 \\| Test Loss: 1.862463510599259 \\| Test Accuracy: 0.5980461275052875\n","Epoch: 191 \\| Time: 0 \\| Train Loss: 2.989144914987013 \\| Test Loss: 1.8231870509012573 \\| Test Accuracy: 0.6385335884781952\n","Epoch: 192 \\| Time: 0 \\| Train Loss: 2.9975783410068635 \\| Test Loss: 1.842750459781532 \\| Test Accuracy: 0.61698056199013\n","Epoch: 193 \\| Time: 0 \\| Train Loss: 3.0046859557826266 \\| Test Loss: 1.8328329299140897 \\| Test Accuracy: 0.6285292241581898\n","Epoch: 194 \\| Time: 0 \\| Train Loss: 2.9869969179240137 \\| Test Loss: 1.8355600265985907 \\| Test Accuracy: 0.6258770604626179\n","Epoch: 195 \\| Time: 0 \\| Train Loss: 2.9948638632718234 \\| Test Loss: 1.8515191247023226 \\| Test Accuracy: 0.6104340819820727\n","Epoch: 196 \\| Time: 0 \\| Train Loss: 2.9974108337853798 \\| Test Loss: 1.8250554881894026 \\| Test Accuracy: 0.6369892906301406\n","Epoch: 197 \\| Time: 0 \\| Train Loss: 3.0034638894793835 \\| Test Loss: 1.845587553896106 \\| Test Accuracy: 0.6137912512169739\n","Epoch: 198 \\| Time: 0 \\| Train Loss: 3.0071848094079394 \\| Test Loss: 1.858457980749433 \\| Test Accuracy: 0.6012018665860946\n","Epoch: 199 \\| Time: 0 \\| Train Loss: 3.0049506573300917 \\| Test Loss: 1.848214580777377 \\| Test Accuracy: 0.6124148118306644\n","Epoch: 200 \\| Time: 0 \\| Train Loss: 2.9743345612816725 \\| Test Loss: 1.827221837166553 \\| Test Accuracy: 0.6335649780105415\n","Epoch: 201 \\| Time: 0 \\| Train Loss: 2.9941964995598322 \\| Test Loss: 1.8289860226054049 \\| Test Accuracy: 0.6332292610870514\n","Epoch: 202 \\| Time: 0 \\| Train Loss: 3.008599919357017 \\| Test Loss: 1.849287426522873 \\| Test Accuracy: 0.6107697989055628\n","Epoch: 203 \\| Time: 0 \\| Train Loss: 3.015082862120364 \\| Test Loss: 1.852053480598548 \\| Test Accuracy: 0.6097962198274415\n","Epoch: 204 \\| Time: 0 \\| Train Loss: 3.014236625777392 \\| Test Loss: 1.83673731707708 \\| Test Accuracy: 0.6246684795380535\n","Epoch: 205 \\| Time: 0 \\| Train Loss: 3.002630890432863 \\| Test Loss: 1.8385555733938586 \\| Test Accuracy: 0.6226541779971129\n","Epoch: 206 \\| Time: 0 \\| Train Loss: 2.992447618078504 \\| Test Loss: 1.8136593927129656 \\| Test Accuracy: 0.6487729546446437\n","Epoch: 207 \\| Time: 0 \\| Train Loss: 3.0096596532899844 \\| Test Loss: 1.8417082042653161 \\| Test Accuracy: 0.6181891429146943\n","Epoch: 208 \\| Time: 0 \\| Train Loss: 3.0206031046971153 \\| Test Loss: 1.8910050330755537 \\| Test Accuracy: 0.5674958874676872\n","Epoch: 209 \\| Time: 0 \\| Train Loss: 3.015558641879845 \\| Test Loss: 1.8533323430196411 \\| Test Accuracy: 0.6058011884379092\n","Epoch: 210 \\| Time: 0 \\| Train Loss: 3.0181109162294386 \\| Test Loss: 1.8344079414662373 \\| Test Accuracy: 0.6266492093866451\n","Epoch: 211 \\| Time: 0 \\| Train Loss: 2.9881649410249276 \\| Test Loss: 1.821815398118015 \\| Test Accuracy: 0.640715748480881\n","Epoch: 212 \\| Time: 0 \\| Train Loss: 2.984513296328525 \\| Test Loss: 1.8395324843124259 \\| Test Accuracy: 0.620136301070937\n","Epoch: 213 \\| Time: 0 \\| Train Loss: 3.0022552646723284 \\| Test Loss: 1.8474607779744356 \\| Test Accuracy: 0.6125155269077114\n","Epoch: 214 \\| Time: 0 \\| Train Loss: 3.014945866981683 \\| Test Loss: 1.833868004221773 \\| Test Accuracy: 0.628965656158727\n","Epoch: 215 \\| Time: 0 \\| Train Loss: 2.993314561317517 \\| Test Loss: 1.8378288336577846 \\| Test Accuracy: 0.6215798838419445\n","Epoch: 216 \\| Time: 0 \\| Train Loss: 2.986646070331713 \\| Test Loss: 1.8453273215519002 \\| Test Accuracy: 0.6166448450666399\n","Epoch: 217 \\| Time: 0 \\| Train Loss: 2.9920283538308365 \\| Test Loss: 1.830367230038786 \\| Test Accuracy: 0.6302749521603384\n","Epoch: 218 \\| Time: 0 \\| Train Loss: 3.014723921298075 \\| Test Loss: 1.8232517892199014 \\| Test Accuracy: 0.6392385940175245\n","Epoch: 219 \\| Time: 0 \\| Train Loss: 2.9918609375897063 \\| Test Loss: 1.8260323827358786 \\| Test Accuracy: 0.6363178567831604\n","Epoch: 220 \\| Time: 0 \\| Train Loss: 3.0129409860544802 \\| Test Loss: 1.848666368124311 \\| Test Accuracy: 0.6137241078322758\n","Epoch: 221 \\| Time: 0 \\| Train Loss: 2.9629888155045734 \\| Test Loss: 1.8345053103860356 \\| Test Accuracy: 0.6256756303085238\n","Epoch: 222 \\| Time: 0 \\| Train Loss: 3.0407661923781877 \\| Test Loss: 1.859055010545919 \\| Test Accuracy: 0.6001947158156242\n","Epoch: 223 \\| Time: 0 \\| Train Loss: 2.997946648011471 \\| Test Loss: 1.8401797907546866 \\| Test Accuracy: 0.6210427367643603\n","Epoch: 224 \\| Time: 0 \\| Train Loss: 3.008267525991713 \\| Test Loss: 1.8325012354380072 \\| Test Accuracy: 0.6287977976969819\n","Epoch: 225 \\| Time: 0 \\| Train Loss: 2.9954081558112304 \\| Test Loss: 1.8344663458320716 \\| Test Accuracy: 0.6267834961560412\n","Epoch: 226 \\| Time: 0 \\| Train Loss: 3.01758854703175 \\| Test Loss: 1.843032135983905 \\| Test Accuracy: 0.6167119884513378\n","Epoch: 227 \\| Time: 0 \\| Train Loss: 3.016321760857546 \\| Test Loss: 1.859079489380505 \\| Test Accuracy: 0.6005304327391143\n","Epoch: 228 \\| Time: 0 \\| Train Loss: 3.033440443660957 \\| Test Loss: 1.8454074394037796 \\| Test Accuracy: 0.6141605398328129\n","Epoch: 229 \\| Time: 0 \\| Train Loss: 3.0319875524963367 \\| Test Loss: 1.8458056076401805 \\| Test Accuracy: 0.6144626850639541\n","Epoch: 230 \\| Time: 0 \\| Train Loss: 2.99944343429012 \\| Test Loss: 1.8339727799779868 \\| Test Accuracy: 0.6274549300030214\n","Epoch: 231 \\| Time: 0 \\| Train Loss: 2.983897490409484 \\| Test Loss: 1.8443359116116307 \\| Test Accuracy: 0.6163426998354987\n","Epoch: 232 \\| Time: 0 \\| Train Loss: 3.006613221380572 \\| Test Loss: 1.8467042814508527 \\| Test Accuracy: 0.6125155269077114\n","Epoch: 233 \\| Time: 0 \\| Train Loss: 3.002241370334465 \\| Test Loss: 1.8258172432240498 \\| Test Accuracy: 0.6358478530902743\n","Epoch: 234 \\| Time: 0 \\| Train Loss: 3.021446676921806 \\| Test Loss: 1.8491669335590413 \\| Test Accuracy: 0.6096619330580455\n","Epoch: 235 \\| Time: 0 \\| Train Loss: 2.9939340103454493 \\| Test Loss: 1.8465386526779044 \\| Test Accuracy: 0.614126968140464\n","Epoch: 236 \\| Time: 0 \\| Train Loss: 2.994220233675078 \\| Test Loss: 1.8635059349526664 \\| Test Accuracy: 0.5983147010440796\n","Epoch: 237 \\| Time: 0 \\| Train Loss: 2.9990064258658764 \\| Test Loss: 1.8327986794991555 \\| Test Accuracy: 0.6276899318494645\n","Epoch: 238 \\| Time: 0 \\| Train Loss: 2.9806061408853894 \\| Test Loss: 1.8215179862894213 \\| Test Accuracy: 0.6405814617114849\n","Epoch: 239 \\| Time: 0 \\| Train Loss: 3.017267789216996 \\| Test Loss: 1.8206870995877638 \\| Test Accuracy: 0.640715748480881\n","Epoch: 240 \\| Time: 0 \\| Train Loss: 3.027330718916883 \\| Test Loss: 1.8376336737252101 \\| Test Accuracy: 0.6226541779971129\n","Epoch: 241 \\| Time: 0 \\| Train Loss: 2.995301156038752 \\| Test Loss: 1.8519268429842117 \\| Test Accuracy: 0.6096619330580455\n","Epoch: 242 \\| Time: 0 \\| Train Loss: 3.0146086668135577 \\| Test Loss: 1.8569857255583668 \\| Test Accuracy: 0.6034511699734784\n","Epoch: 243 \\| Time: 0 \\| Train Loss: 2.9926491446034 \\| Test Loss: 1.856451198266811 \\| Test Accuracy: 0.6068419109007285\n","Epoch: 244 \\| Time: 0 \\| Train Loss: 3.014833657805275 \\| Test Loss: 1.8308205451064867 \\| Test Accuracy: 0.6308792426226206\n","Epoch: 245 \\| Time: 0 \\| Train Loss: 3.008218202900469 \\| Test Loss: 1.8257070037940029 \\| Test Accuracy: 0.635042132473898\n","Epoch: 246 \\| Time: 0 \\| Train Loss: 2.993908450705446 \\| Test Loss: 1.8569400310516357 \\| Test Accuracy: 0.6037533152046195\n","Epoch: 247 \\| Time: 0 \\| Train Loss: 2.9871012995565316 \\| Test Loss: 1.8312211456216967 \\| Test Accuracy: 0.6289320844663779\n","Epoch: 248 \\| Time: 0 \\| Train Loss: 3.0026825259537553 \\| Test Loss: 1.8417803980250216 \\| Test Accuracy: 0.6191627219928156\n","Epoch: 249 \\| Time: 0 \\| Train Loss: 3.0080861383798476 \\| Test Loss: 1.8434801106800849 \\| Test Accuracy: 0.6170477053748279\n","Epoch: 250 \\| Time: 0 \\| Train Loss: 3.0053203152677126 \\| Test Loss: 1.8470500635998444 \\| Test Accuracy: 0.6132541041393896\n","Epoch: 251 \\| Time: 0 \\| Train Loss: 3.001890960230137 \\| Test Loss: 1.8445717725630995 \\| Test Accuracy: 0.6151005472185853\n","Epoch: 252 \\| Time: 0 \\| Train Loss: 2.9924986888507386 \\| Test Loss: 1.8282532620327667 \\| Test Accuracy: 0.6331285460100043\n","Epoch: 253 \\| Time: 0 \\| Train Loss: 3.002530671070674 \\| Test Loss: 1.8503331401317415 \\| Test Accuracy: 0.6091919293651593\n","Epoch: 254 \\| Time: 0 \\| Train Loss: 2.9942273927085203 \\| Test Loss: 1.8190875570149891 \\| Test Accuracy: 0.643871487561688\n","Epoch: 255 \\| Time: 0 \\| Train Loss: 3.0002313501464126 \\| Test Loss: 1.849404797533551 \\| Test Accuracy: 0.611306945983147\n","Epoch: 256 \\| Time: 0 \\| Train Loss: 2.996693056640773 \\| Test Loss: 1.838189990735361 \\| Test Accuracy: 0.6227884647665088\n","Epoch: 257 \\| Time: 0 \\| Train Loss: 2.980139798055306 \\| Test Loss: 1.8601043459683528 \\| Test Accuracy: 0.599254708429852\n","Epoch: 258 \\| Time: 0 \\| Train Loss: 3.0378828200631496 \\| Test Loss: 1.8285806189279188 \\| Test Accuracy: 0.6326921140094672\n","Epoch: 259 \\| Time: 0 \\| Train Loss: 2.9958756436244403 \\| Test Loss: 1.8234962450076582 \\| Test Accuracy: 0.6393393090945715\n","Epoch: 260 \\| Time: 0 \\| Train Loss: 2.982598565168539 \\| Test Loss: 1.8124013305222015 \\| Test Accuracy: 0.6490079564910867\n","Epoch: 261 \\| Time: 0 \\| Train Loss: 3.0049393622797718 \\| Test Loss: 1.8257103754215487 \\| Test Accuracy: 0.635109275858596\n","Epoch: 262 \\| Time: 0 \\| Train Loss: 3.0084114387723244 \\| Test Loss: 1.8839897255017521 \\| Test Accuracy: 0.5745459428609796\n","Epoch: 263 \\| Time: 0 \\| Train Loss: 2.998466619641925 \\| Test Loss: 1.8337556205593977 \\| Test Accuracy: 0.6276227884647665\n","Epoch: 264 \\| Time: 0 \\| Train Loss: 2.9856801290306008 \\| Test Loss: 1.859867274505386 \\| Test Accuracy: 0.6002954308926713\n","Epoch: 265 \\| Time: 0 \\| Train Loss: 3.0354489491511796 \\| Test Loss: 1.8698531205050424 \\| Test Accuracy: 0.5892839158021956\n","Epoch: 266 \\| Time: 0 \\| Train Loss: 3.0156365247934978 \\| Test Loss: 1.8273532922687448 \\| Test Accuracy: 0.6331285460100043\n","Epoch: 267 \\| Time: 0 \\| Train Loss: 2.9887982433694553 \\| Test Loss: 1.819828229912361 \\| Test Accuracy: 0.6427971934065196\n","Epoch: 268 \\| Time: 0 \\| Train Loss: 3.0280638957256976 \\| Test Loss: 1.901574693012647 \\| Test Accuracy: 0.5582636720717091\n","Epoch: 269 \\| Time: 0 \\| Train Loss: 3.019645673966331 \\| Test Loss: 1.8816749016102803 \\| Test Accuracy: 0.5772316782489005\n","Epoch: 270 \\| Time: 0 \\| Train Loss: 3.011703090280151 \\| Test Loss: 1.821975169775312 \\| Test Accuracy: 0.6396078826333635\n","Epoch: 271 \\| Time: 0 \\| Train Loss: 3.001965788459604 \\| Test Loss: 1.8270718636942525 \\| Test Accuracy: 0.6330278309329573\n","Epoch: 272 \\| Time: 0 \\| Train Loss: 2.989447581238795 \\| Test Loss: 1.8459190160931436 \\| Test Accuracy: 0.6136233927552288\n","Epoch: 273 \\| Time: 0 \\| Train Loss: 2.9895501516256924 \\| Test Loss: 1.8495883153743498 \\| Test Accuracy: 0.6104340819820727\n","Epoch: 274 \\| Time: 0 \\| Train Loss: 2.9921922907015763 \\| Test Loss: 1.8314938519645658 \\| Test Accuracy: 0.6285627958505389\n","Epoch: 275 \\| Time: 0 \\| Train Loss: 3.018495547073285 \\| Test Loss: 1.8221401820366987 \\| Test Accuracy: 0.6393728807869205\n","Epoch: 276 \\| Time: 0 \\| Train Loss: 3.0466709234979144 \\| Test Loss: 1.8622047404874549 \\| Test Accuracy: 0.5977439822741465\n","Epoch: 277 \\| Time: 0 \\| Train Loss: 2.9627403868009097 \\| Test Loss: 1.857528459872299 \\| Test Accuracy: 0.60237687581831\n","Epoch: 278 \\| Time: 0 \\| Train Loss: 2.994187892649177 \\| Test Loss: 1.829882005253575 \\| Test Accuracy: 0.6306442407761775\n","Epoch: 279 \\| Time: 0 \\| Train Loss: 2.9961904485834134 \\| Test Loss: 1.8199923064064059 \\| Test Accuracy: 0.6423943330983315\n","Epoch: 280 \\| Time: 0 \\| Train Loss: 2.9824900977225632 \\| Test Loss: 1.8455568930621822 \\| Test Accuracy: 0.6149998321415383\n","Epoch: 281 \\| Time: 0 \\| Train Loss: 3.006321206883983 \\| Test Loss: 1.8397117418281 \\| Test Accuracy: 0.6219491724577836\n","Epoch: 282 \\| Time: 0 \\| Train Loss: 2.989612927910921 \\| Test Loss: 1.846295181773763 \\| Test Accuracy: 0.6152684056803304\n","Epoch: 283 \\| Time: 0 \\| Train Loss: 3.0160911686131655 \\| Test Loss: 1.8324981406011296 \\| Test Accuracy: 0.6288649410816799\n","Epoch: 284 \\| Time: 0 \\| Train Loss: 3.001487369087753 \\| Test Loss: 1.83057732796976 \\| Test Accuracy: 0.6307113841608756\n","Epoch: 285 \\| Time: 0 \\| Train Loss: 2.9944566382713953 \\| Test Loss: 1.8776809846894424 \\| Test Accuracy: 0.5831402961023265\n","Epoch: 286 \\| Time: 0 \\| Train Loss: 3.02536007596519 \\| Test Loss: 1.8578266237938352 \\| Test Accuracy: 0.6018733004330749\n","Epoch: 287 \\| Time: 0 \\| Train Loss: 2.9933031180176597 \\| Test Loss: 1.840785195899112 \\| Test Accuracy: 0.6218820290730855\n","Epoch: 288 \\| Time: 0 \\| Train Loss: 2.9697566330232137 \\| Test Loss: 1.8334885723089456 \\| Test Accuracy: 0.6271192130795313\n","Epoch: 289 \\| Time: 0 \\| Train Loss: 3.00161350326415 \\| Test Loss: 1.8473966750975843 \\| Test Accuracy: 0.614227683217511\n","Epoch: 290 \\| Time: 0 \\| Train Loss: 2.9917459953641465 \\| Test Loss: 1.8303675298527076 \\| Test Accuracy: 0.6310135293920166\n","Epoch: 291 \\| Time: 0 \\| Train Loss: 2.987496220849347 \\| Test Loss: 1.8467626525608767 \\| Test Accuracy: 0.6154698358344244\n","Epoch: 292 \\| Time: 0 \\| Train Loss: 2.9845452147309297 \\| Test Loss: 1.8274860714638181 \\| Test Accuracy: 0.6341356967804747\n","Epoch: 293 \\| Time: 0 \\| Train Loss: 3.0054663386070604 \\| Test Loss: 1.8348453745821516 \\| Test Accuracy: 0.6267163527713432\n","Epoch: 294 \\| Time: 0 \\| Train Loss: 2.9969231006137242 \\| Test Loss: 1.8536349529872125 \\| Test Accuracy: 0.6055661865914661\n","Epoch: 295 \\| Time: 0 \\| Train Loss: 2.9765830303160237 \\| Test Loss: 1.8576328483262288 \\| Test Accuracy: 0.6019740155101219\n","Epoch: 296 \\| Time: 0 \\| Train Loss: 3.0194473626004683 \\| Test Loss: 1.857551883730254 \\| Test Accuracy: 0.6040218887434116\n","Epoch: 297 \\| Time: 0 \\| Train Loss: 3.015152336155802 \\| Test Loss: 1.8481457965056785 \\| Test Accuracy: 0.6132541041393896\n","Epoch: 298 \\| Time: 0 \\| Train Loss: 2.995056055745602 \\| Test Loss: 1.8209476025831035 \\| Test Accuracy: 0.6397757410951086\n","Epoch: 299 \\| Time: 0 \\| Train Loss: 3.001074444456101 \\| Test Loss: 1.8232444146160405 \\| Test Accuracy: 0.6383321583241012\n","Epoch: 300 \\| Time: 0 \\| Train Loss: 3.007535793030458 \\| Test Loss: 1.8535116650004244 \\| Test Accuracy: 0.6068083392083795\n","Epoch: 301 \\| Time: 0 \\| Train Loss: 3.008607931184066 \\| Test Loss: 1.8496019471868426 \\| Test Accuracy: 0.6124148118306644\n","Epoch: 302 \\| Time: 0 \\| Train Loss: 3.0109916988269303 \\| Test Loss: 1.8854436204157161 \\| Test Accuracy: 0.5735387920905093\n","Epoch: 303 \\| Time: 0 \\| Train Loss: 3.029994106531542 \\| Test Loss: 1.8345614518218798 \\| Test Accuracy: 0.626246349078457\n","Epoch: 304 \\| Time: 0 \\| Train Loss: 3.000928698591796 \\| Test Loss: 1.8362323040614312 \\| Test Accuracy: 0.6236277570752342\n","Epoch: 305 \\| Time: 0 \\| Train Loss: 2.9966529492729226 \\| Test Loss: 1.8339508234687119 \\| Test Accuracy: 0.6277235035418135\n","Epoch: 306 \\| Time: 0 \\| Train Loss: 3.035689197793303 \\| Test Loss: 1.8722187024840982 \\| Test Accuracy: 0.5860610333366905\n","Epoch: 307 \\| Time: 0 \\| Train Loss: 2.98776662465783 \\| Test Loss: 1.844838054395029 \\| Test Accuracy: 0.6156712659885185\n","Epoch: 308 \\| Time: 0 \\| Train Loss: 2.9930932373055676 \\| Test Loss: 1.8427047166701551 \\| Test Accuracy: 0.6161412696814046\n","Epoch: 309 \\| Time: 0 \\| Train Loss: 3.022076459602862 \\| Test Loss: 1.853533788812007 \\| Test Accuracy: 0.6099640782891865\n","Epoch: 310 \\| Time: 0 \\| Train Loss: 2.9832713710501944 \\| Test Loss: 1.8372758222751864 \\| Test Accuracy: 0.6236613287675832\n","Epoch: 311 \\| Time: 0 \\| Train Loss: 3.005985541321018 \\| Test Loss: 1.8368204627425886 \\| Test Accuracy: 0.6244334776916104\n","Epoch: 312 \\| Time: 0 \\| Train Loss: 2.9833179307781013 \\| Test Loss: 1.847389924679703 \\| Test Accuracy: 0.6111055158290529\n","Epoch: 313 \\| Time: 0 \\| Train Loss: 3.059292056085069 \\| Test Loss: 1.8692911198210818 \\| Test Accuracy: 0.5893846308792426\n","Epoch: 314 \\| Time: 0 \\| Train Loss: 2.990600727024729 \\| Test Loss: 1.8352782654659943 \\| Test Accuracy: 0.6254070567697317\n","Epoch: 315 \\| Time: 0 \\| Train Loss: 2.981582657367759 \\| Test Loss: 1.8321604815675465 \\| Test Accuracy: 0.629032799543425\n","Epoch: 316 \\| Time: 0 \\| Train Loss: 2.979314477074827 \\| Test Loss: 1.8196233122133902 \\| Test Accuracy: 0.6422264746365864\n","Epoch: 317 \\| Time: 0 \\| Train Loss: 2.9764752651964788 \\| Test Loss: 1.8363948021835523 \\| Test Accuracy: 0.6246684795380535\n","Epoch: 318 \\| Time: 0 \\| Train Loss: 2.992285082554198 \\| Test Loss: 1.8575114589903998 \\| Test Accuracy: 0.602578305972404\n","Epoch: 319 \\| Time: 0 \\| Train Loss: 2.995954055561027 \\| Test Loss: 1.8291091985456933 \\| Test Accuracy: 0.6333635478564474\n","Epoch: 320 \\| Time: 0 \\| Train Loss: 3.0278217844741944 \\| Test Loss: 1.8335217094216736 \\| Test Accuracy: 0.6298720918521503\n","Epoch: 321 \\| Time: 0 \\| Train Loss: 3.0070988211776757 \\| Test Loss: 1.8567759980459582 \\| Test Accuracy: 0.6028804512035452\n","Epoch: 322 \\| Time: 0 \\| Train Loss: 3.0289324401147475 \\| Test Loss: 1.8672125651601046 \\| Test Accuracy: 0.5916675059589754\n","Epoch: 323 \\| Time: 0 \\| Train Loss: 2.995503353268588 \\| Test Loss: 1.8637162175812947 \\| Test Accuracy: 0.5956625373485077\n","Epoch: 324 \\| Time: 0 \\| Train Loss: 3.0104979410709327 \\| Test Loss: 1.8471295941029495 \\| Test Accuracy: 0.6135898210628797\n","Epoch: 325 \\| Time: 0 \\| Train Loss: 2.992281371318332 \\| Test Loss: 1.82768524818666 \\| Test Accuracy: 0.6333299761640985\n","Epoch: 326 \\| Time: 0 \\| Train Loss: 2.9959321634539164 \\| Test Loss: 1.826812391117407 \\| Test Accuracy: 0.6361835700137644\n","Epoch: 327 \\| Time: 0 \\| Train Loss: 2.99528391224308 \\| Test Loss: 1.8214485133666338 \\| Test Accuracy: 0.6421257595595394\n","Epoch: 328 \\| Time: 0 \\| Train Loss: 2.99048475008372 \\| Test Loss: 1.8124691048405202 \\| Test Accuracy: 0.6513244032631685\n","Epoch: 329 \\| Time: 0 \\| Train Loss: 2.9918392872596504 \\| Test Loss: 1.903241618508433 \\| Test Accuracy: 0.5542350689898278\n","Epoch: 330 \\| Time: 0 \\| Train Loss: 2.9916624108949637 \\| Test Loss: 1.8343402474734916 \\| Test Accuracy: 0.6264813509249001\n","Epoch: 331 \\| Time: 0 \\| Train Loss: 2.9761660916647426 \\| Test Loss: 1.825323649742061 \\| Test Accuracy: 0.6372578641689327\n","Epoch: 332 \\| Time: 0 \\| Train Loss: 2.97450733942439 \\| Test Loss: 1.8277602871088512 \\| Test Accuracy: 0.6347064155504079\n","Epoch: 333 \\| Time: 0 \\| Train Loss: 2.9733465074840066 \\| Test Loss: 1.8322510867671393 \\| Test Accuracy: 0.628999227851076\n","Epoch: 334 \\| Time: 0 \\| Train Loss: 2.9923311322525494 \\| Test Loss: 1.8335581449991645 \\| Test Accuracy: 0.6278242186188606\n","Epoch: 335 \\| Time: 0 \\| Train Loss: 2.986525786336572 \\| Test Loss: 1.8358944644232165 \\| Test Accuracy: 0.6249034813844966\n","Epoch: 336 \\| Time: 0 \\| Train Loss: 2.9873965248201007 \\| Test Loss: 1.8421267828716228 \\| Test Accuracy: 0.6192298653775137\n","Epoch: 337 \\| Time: 0 \\| Train Loss: 3.0142149732467827 \\| Test Loss: 1.8291317991944342 \\| Test Accuracy: 0.6344042703192668\n","Epoch: 338 \\| Time: 0 \\| Train Loss: 2.991653406745966 \\| Test Loss: 1.8362597053143088 \\| Test Accuracy: 0.6246013361533554\n","Epoch: 339 \\| Time: 0 \\| Train Loss: 2.9716387576547567 \\| Test Loss: 1.8309516543482507 \\| Test Accuracy: 0.6287306543122839\n","Epoch: 340 \\| Time: 0 \\| Train Loss: 2.967291764746345 \\| Test Loss: 1.8443763675607836 \\| Test Accuracy: 0.6170141336824789\n","Epoch: 341 \\| Time: 0 \\| Train Loss: 3.0093278211105052 \\| Test Loss: 1.8400261611897546 \\| Test Accuracy: 0.6214455970725484\n","Epoch: 342 \\| Time: 0 \\| Train Loss: 2.97421329047471 \\| Test Loss: 1.8465871319750349 \\| Test Accuracy: 0.6145634001410011\n","Epoch: 343 \\| Time: 0 \\| Train Loss: 3.006642216161255 \\| Test Loss: 1.8538934534711387 \\| Test Accuracy: 0.6060361902843522\n","Epoch: 344 \\| Time: 0 \\| Train Loss: 2.9982451495493976 \\| Test Loss: 1.824109888383759 \\| Test Accuracy: 0.6390371638634303\n","Epoch: 345 \\| Time: 0 \\| Train Loss: 2.9840426047119326 \\| Test Loss: 1.8436712282409995 \\| Test Accuracy: 0.6165777016819418\n","Epoch: 346 \\| Time: 0 \\| Train Loss: 2.9940521114326537 \\| Test Loss: 1.8144835126246506 \\| Test Accuracy: 0.6485043811058515\n","Epoch: 347 \\| Time: 0 \\| Train Loss: 3.023968742933058 \\| Test Loss: 1.85186730178129 \\| Test Accuracy: 0.6075469164400578\n","Epoch: 348 \\| Time: 0 \\| Train Loss: 2.9963810725592235 \\| Test Loss: 1.827389152264902 \\| Test Accuracy: 0.6346392721657099\n","Epoch: 349 \\| Time: 0 \\| Train Loss: 3.021683661167056 \\| Test Loss: 1.8233752337648121 \\| Test Accuracy: 0.6391043072481284\n","Epoch: 350 \\| Time: 0 \\| Train Loss: 2.9872088930695306 \\| Test Loss: 1.8210977428460837 \\| Test Accuracy: 0.6416893275590022\n","Epoch: 351 \\| Time: 0 \\| Train Loss: 2.981694999077582 \\| Test Loss: 1.8165202151040662 \\| Test Accuracy: 0.646725081411354\n","Epoch: 352 \\| Time: 0 \\| Train Loss: 2.999270010279636 \\| Test Loss: 1.8519966233953387 \\| Test Accuracy: 0.6072447712089166\n","Epoch: 353 \\| Time: 0 \\| Train Loss: 2.9960261176256986 \\| Test Loss: 1.832930599671065 \\| Test Accuracy: 0.6294356598516131\n","Epoch: 354 \\| Time: 0 \\| Train Loss: 3.0077654760631276 \\| Test Loss: 1.8677862186800256 \\| Test Accuracy: 0.5913989324201833\n","Epoch: 355 \\| Time: 0 \\| Train Loss: 2.9833026897642516 \\| Test Loss: 1.81645169943699 \\| Test Accuracy: 0.644039346023433\n","Epoch: 356 \\| Time: 0 \\| Train Loss: 3.019441757328366 \\| Test Loss: 1.8429447212955983 \\| Test Accuracy: 0.6168127035283849\n","Epoch: 357 \\| Time: 0 \\| Train Loss: 3.0547816215146755 \\| Test Loss: 1.9001779279995374 \\| Test Accuracy: 0.5591029643804344\n","Epoch: 358 \\| Time: 0 \\| Train Loss: 3.019249844994216 \\| Test Loss: 1.852042493390423 \\| Test Accuracy: 0.6089904992110652\n","Epoch: 359 \\| Time: 0 \\| Train Loss: 3.0102433459227065 \\| Test Loss: 1.8290910367801978 \\| Test Accuracy: 0.6317185349313459\n","Epoch: 360 \\| Time: 0 \\| Train Loss: 3.0163170942810176 \\| Test Loss: 1.860380488403877 \\| Test Accuracy: 0.5978782690435425\n","Epoch: 361 \\| Time: 0 \\| Train Loss: 3.0170435627566006 \\| Test Loss: 1.8265917736061652 \\| Test Accuracy: 0.6338335515493336\n","Epoch: 362 \\| Time: 0 \\| Train Loss: 2.999592119577916 \\| Test Loss: 1.826222539459687 \\| Test Accuracy: 0.6345385570886628\n","Epoch: 363 \\| Time: 0 \\| Train Loss: 3.0213371001307188 \\| Test Loss: 1.8416157500426658 \\| Test Accuracy: 0.6185248598381844\n","Epoch: 364 \\| Time: 0 \\| Train Loss: 2.982841548029258 \\| Test Loss: 1.8263778374430448 \\| Test Accuracy: 0.6338335515493336\n","Epoch: 365 \\| Time: 0 \\| Train Loss: 2.987918758278314 \\| Test Loss: 1.8434041451998535 \\| Test Accuracy: 0.6162419847584517\n","Epoch: 366 \\| Time: 0 \\| Train Loss: 3.0096456715877156 \\| Test Loss: 1.8638363518940022 \\| Test Accuracy: 0.5951253902709236\n","Epoch: 367 \\| Time: 0 \\| Train Loss: 3.0104632931796256 \\| Test Loss: 1.855137780500584 \\| Test Accuracy: 0.605230469667976\n","Epoch: 368 \\| Time: 0 \\| Train Loss: 3.000753328940328 \\| Test Loss: 1.837194167493239 \\| Test Accuracy: 0.6241984758451673\n","Epoch: 369 \\| Time: 0 \\| Train Loss: 2.991013879432953 \\| Test Loss: 1.845269944023165 \\| Test Accuracy: 0.6161748413737537\n","Epoch: 370 \\| Time: 0 \\| Train Loss: 2.981133664918596 \\| Test Loss: 1.8426020048206968 \\| Test Accuracy: 0.61698056199013\n","Epoch: 371 \\| Time: 0 \\| Train Loss: 3.005865062621814 \\| Test Loss: 1.842847030050253 \\| Test Accuracy: 0.6170141336824789\n","Epoch: 372 \\| Time: 0 \\| Train Loss: 2.9839758623085486 \\| Test Loss: 1.8437154707478862 \\| Test Accuracy: 0.61698056199013\n","Epoch: 373 \\| Time: 0 \\| Train Loss: 3.015203676470101 \\| Test Loss: 1.8253837900611976 \\| Test Accuracy: 0.6365864303219525\n","Epoch: 374 \\| Time: 0 \\| Train Loss: 3.0530496468130903 \\| Test Loss: 1.8716598626370082 \\| Test Accuracy: 0.5877396179541411\n","Epoch: 375 \\| Time: 0 \\| Train Loss: 2.98522936877162 \\| Test Loss: 1.8435324921628435 \\| Test Accuracy: 0.6176855675294591\n","Epoch: 376 \\| Time: 0 \\| Train Loss: 2.996112004855023 \\| Test Loss: 1.8324584638611954 \\| Test Accuracy: 0.6273206432336254\n","Epoch: 377 \\| Time: 0 \\| Train Loss: 2.9743016261983155 \\| Test Loss: 1.838117785719843 \\| Test Accuracy: 0.623292040151744\n","Epoch: 378 \\| Time: 0 \\| Train Loss: 2.986902572717449 \\| Test Loss: 1.8572417103681442 \\| Test Accuracy: 0.6037197435122704\n","Epoch: 379 \\| Time: 0 \\| Train Loss: 3.0400083286876263 \\| Test Loss: 1.8872825055674933 \\| Test Accuracy: 0.5721623527041998\n","Epoch: 380 \\| Time: 0 \\| Train Loss: 3.0163377040582176 \\| Test Loss: 1.8269481740796003 \\| Test Accuracy: 0.6337328364722866\n","Epoch: 381 \\| Time: 0 \\| Train Loss: 3.0025330883581596 \\| Test Loss: 1.8453176717389808 \\| Test Accuracy: 0.6148319736797933\n","Epoch: 382 \\| Time: 0 \\| Train Loss: 3.0182869283090725 \\| Test Loss: 1.8788786834913263 \\| Test Accuracy: 0.5818645717930641\n","Epoch: 383 \\| Time: 0 \\| Train Loss: 2.97716678217897 \\| Test Loss: 1.8183359419327436 \\| Test Accuracy: 0.6434014838688018\n","Epoch: 384 \\| Time: 0 \\| Train Loss: 2.989760629201985 \\| Test Loss: 1.8299866541260814 \\| Test Accuracy: 0.631852821700742\n","Epoch: 385 \\| Time: 0 \\| Train Loss: 3.0154342528208797 \\| Test Loss: 1.8258229289443708 \\| Test Accuracy: 0.6343035552422197\n","Epoch: 386 \\| Time: 0 \\| Train Loss: 2.9864086584748017 \\| Test Loss: 1.8551278774318778 \\| Test Accuracy: 0.6041561755128076\n","Epoch: 387 \\| Time: 0 \\| Train Loss: 3.0093743535497084 \\| Test Loss: 1.8327494869927992 \\| Test Accuracy: 0.6274213583106725\n","Epoch: 388 \\| Time: 0 \\| Train Loss: 3.0019147216008766 \\| Test Loss: 1.8391069966836038 \\| Test Accuracy: 0.6221841743042267\n","Epoch: 389 \\| Time: 0 \\| Train Loss: 3.0072269845773265 \\| Test Loss: 1.8532042017310475 \\| Test Accuracy: 0.6078154899788498\n","Epoch: 390 \\| Time: 0 \\| Train Loss: 3.0084060675434054 \\| Test Loss: 1.8495676573765636 \\| Test Accuracy: 0.6105347970591197\n","Epoch: 391 \\| Time: 0 \\| Train Loss: 2.9818644422671383 \\| Test Loss: 1.831995856608444 \\| Test Accuracy: 0.629267801389868\n","Epoch: 392 \\| Time: 0 \\| Train Loss: 3.0035892593640154 \\| Test Loss: 1.8421125718964015 \\| Test Accuracy: 0.617249135528922\n","Epoch: 393 \\| Time: 0 \\| Train Loss: 3.0038883153559914 \\| Test Loss: 1.8273508231527305 \\| Test Accuracy: 0.631987108470138\n","Epoch: 394 \\| Time: 0 \\| Train Loss: 2.9993141832941985 \\| Test Loss: 1.835357894201647 \\| Test Accuracy: 0.6247691946151005\n","Epoch: 395 \\| Time: 0 \\| Train Loss: 3.002928943619861 \\| Test Loss: 1.8236803819181582 \\| Test Accuracy: 0.637929298015913\n","Epoch: 396 \\| Time: 0 \\| Train Loss: 2.9969521807766286 \\| Test Loss: 1.8408788442611694 \\| Test Accuracy: 0.6205391613791251\n","Epoch: 397 \\| Time: 0 \\| Train Loss: 2.977723375622795 \\| Test Loss: 1.85550614334483 \\| Test Accuracy: 0.6041561755128076\n","Epoch: 398 \\| Time: 0 \\| Train Loss: 3.0386613052209124 \\| Test Loss: 1.8426476154204603 \\| Test Accuracy: 0.617148420451875\n","Epoch: 399 \\| Time: 0 \\| Train Loss: 3.009596890553634 \\| Test Loss: 1.876160626247717 \\| Test Accuracy: 0.5830395810252795\n","Epoch: 400 \\| Time: 0 \\| Train Loss: 2.99056734393307 \\| Test Loss: 1.8319282900110334 \\| Test Accuracy: 0.6282942223117467\n","Epoch: 401 \\| Time: 0 \\| Train Loss: 3.025134281561527 \\| Test Loss: 1.8342509341342255 \\| Test Accuracy: 0.6265820660019472\n","Epoch: 402 \\| Time: 0 \\| Train Loss: 3.0241647428613954 \\| Test Loss: 1.8256157887340103 \\| Test Accuracy: 0.6360828549367173\n","Epoch: 403 \\| Time: 0 \\| Train Loss: 3.014075983908165 \\| Test Loss: 1.828069023819952 \\| Test Accuracy: 0.634941417396851\n","Epoch: 404 \\| Time: 0 \\| Train Loss: 2.9981125815150085 \\| Test Loss: 1.8453857100572708 \\| Test Accuracy: 0.6153019773726793\n","Epoch: 405 \\| Time: 0 \\| Train Loss: 3.0133074032546308 \\| Test Loss: 1.8528527277222007 \\| Test Accuracy: 0.6064054789001914\n","Epoch: 406 \\| Time: 0 \\| Train Loss: 3.0439020571066138 \\| Test Loss: 1.8133257632603461 \\| Test Accuracy: 0.6486722395675966\n","Epoch: 407 \\| Time: 0 \\| Train Loss: 3.0276518532592083 \\| Test Loss: 1.8325055730189377 \\| Test Accuracy: 0.6274549300030214\n","Epoch: 408 \\| Time: 0 \\| Train Loss: 3.0087763396673544 \\| Test Loss: 1.815186584967912 \\| Test Accuracy: 0.6483700943364555\n","Epoch: 409 \\| Time: 0 \\| Train Loss: 3.0277679390956163 \\| Test Loss: 1.8609160409976484 \\| Test Accuracy: 0.5982139859670326\n","Epoch: 410 \\| Time: 0 \\| Train Loss: 3.015299356231512 \\| Test Loss: 1.8261745109066942 \\| Test Accuracy: 0.6344378420116158\n","Epoch: 411 \\| Time: 0 \\| Train Loss: 2.9834546734853298 \\| Test Loss: 1.835506461208982 \\| Test Accuracy: 0.626414207540202\n","Epoch: 412 \\| Time: 0 \\| Train Loss: 3.020506210778265 \\| Test Loss: 1.8272253085615298 \\| Test Accuracy: 0.6357135663208783\n","Epoch: 413 \\| Time: 0 \\| Train Loss: 3.023341432982148 \\| Test Loss: 1.8324411058630554 \\| Test Accuracy: 0.6300735220062443\n","Epoch: 414 \\| Time: 0 \\| Train Loss: 3.0048168110477205 \\| Test Loss: 1.8535831655043902 \\| Test Accuracy: 0.605230469667976\n","Epoch: 415 \\| Time: 0 \\| Train Loss: 3.002186699398939 \\| Test Loss: 1.8410620638229305 \\| Test Accuracy: 0.620102729378588\n","Epoch: 416 \\| Time: 0 \\| Train Loss: 3.0246519590563676 \\| Test Loss: 1.8530636943972674 \\| Test Accuracy: 0.6073119145936147\n","Epoch: 417 \\| Time: 0 \\| Train Loss: 3.013905916642653 \\| Test Loss: 1.9040048327057146 \\| Test Accuracy: 0.5551079329909021\n","Epoch: 418 \\| Time: 0 \\| Train Loss: 3.025496185305242 \\| Test Loss: 1.8818795880534618 \\| Test Accuracy: 0.5778023970188337\n","Epoch: 419 \\| Time: 0 \\| Train Loss: 2.9859067599394913 \\| Test Loss: 1.8655818465441594 \\| Test Accuracy: 0.5945210998086413\n","Epoch: 420 \\| Time: 0 \\| Train Loss: 2.991437913846696 \\| Test Loss: 1.865644387932806 \\| Test Accuracy: 0.5930103736529359\n","Epoch: 421 \\| Time: 0 \\| Train Loss: 2.981166581630581 \\| Test Loss: 1.8500388748144387 \\| Test Accuracy: 0.6099976499815356\n","Epoch: 422 \\| Time: 0 \\| Train Loss: 3.009758219723197 \\| Test Loss: 1.8341504059124403 \\| Test Accuracy: 0.6281263638500016\n","Epoch: 423 \\| Time: 0 \\| Train Loss: 3.0015958307312367 \\| Test Loss: 1.831764007330964 \\| Test Accuracy: 0.6285963675428878\n","Epoch: 424 \\| Time: 0 \\| Train Loss: 2.977697687991607 \\| Test Loss: 1.8161978639758196 \\| Test Accuracy: 0.6459193607949777\n","Epoch: 425 \\| Time: 0 \\| Train Loss: 3.0349904438736184 \\| Test Loss: 1.8534839301661872 \\| Test Accuracy: 0.6070097693624735\n","Epoch: 426 \\| Time: 0 \\| Train Loss: 3.0031659018655508 \\| Test Loss: 1.8339368780283458 \\| Test Accuracy: 0.6275220733877195\n","Epoch: 427 \\| Time: 0 \\| Train Loss: 3.000505471294479 \\| Test Loss: 1.8361471794193907 \\| Test Accuracy: 0.6244334776916104\n","Epoch: 428 \\| Time: 0 \\| Train Loss: 3.002709432660181 \\| Test Loss: 1.8415825796741272 \\| Test Accuracy: 0.6178869976835533\n","Epoch: 429 \\| Time: 0 \\| Train Loss: 3.0172161617727893 \\| Test Loss: 1.854326176029418 \\| Test Accuracy: 0.6046261792056937\n","Epoch: 430 \\| Time: 0 \\| Train Loss: 2.9772495306776188 \\| Test Loss: 1.853609770152702 \\| Test Accuracy: 0.6064726222848894\n","Epoch: 431 \\| Time: 0 \\| Train Loss: 2.990961980747315 \\| Test Loss: 1.861715763423576 \\| Test Accuracy: 0.5985161311981737\n","Epoch: 432 \\| Time: 0 \\| Train Loss: 3.0115879251206974 \\| Test Loss: 1.841679562826525 \\| Test Accuracy: 0.6190620069157686\n","Epoch: 433 \\| Time: 0 \\| Train Loss: 2.9958351169561004 \\| Test Loss: 1.8417007974289006 \\| Test Accuracy: 0.6179205693759022\n","Epoch: 434 \\| Time: 0 \\| Train Loss: 2.9798837927015747 \\| Test Loss: 1.8445703696795288 \\| Test Accuracy: 0.6171148487595259\n","Epoch: 435 \\| Time: 0 \\| Train Loss: 3.016408220932354 \\| Test Loss: 1.812762816576487 \\| Test Accuracy: 0.6486722395675966\n","Epoch: 436 \\| Time: 0 \\| Train Loss: 2.98342756420416 \\| Test Loss: 1.8199820298493676 \\| Test Accuracy: 0.6427971934065196\n","Epoch: 437 \\| Time: 0 \\| Train Loss: 2.9638691300021893 \\| Test Loss: 1.8317551950528386 \\| Test Accuracy: 0.6280927921576527\n","Epoch: 438 \\| Time: 0 \\| Train Loss: 2.994747298311156 \\| Test Loss: 1.8266923780605004 \\| Test Accuracy: 0.6332628327794004\n","Epoch: 439 \\| Time: 0 \\| Train Loss: 2.9835983109288784 \\| Test Loss: 1.8286658991048264 \\| Test Accuracy: 0.632154966931883\n","Epoch: 440 \\| Time: 0 \\| Train Loss: 3.0074903963985204 \\| Test Loss: 1.8299484406418043 \\| Test Accuracy: 0.6310471010843657\n","Epoch: 441 \\| Time: 0 \\| Train Loss: 3.0073374128012396 \\| Test Loss: 1.8729190974788092 \\| Test Accuracy: 0.5880081914929332\n","Epoch: 442 \\| Time: 0 \\| Train Loss: 3.019942763973937 \\| Test Loss: 1.8401464638280254 \\| Test Accuracy: 0.6182227146070434\n","Epoch: 443 \\| Time: 0 \\| Train Loss: 2.9833858455796287 \\| Test Loss: 1.83197783502898 \\| Test Accuracy: 0.6294020881592641\n","Epoch: 444 \\| Time: 0 \\| Train Loss: 3.0045067199209066 \\| Test Loss: 1.854287301521956 \\| Test Accuracy: 0.6074797730553597\n","Epoch: 445 \\| Time: 0 \\| Train Loss: 2.990091724308307 \\| Test Loss: 1.8409492370908351 \\| Test Accuracy: 0.6208748783026152\n","Epoch: 446 \\| Time: 0 \\| Train Loss: 2.997067067696422 \\| Test Loss: 1.8254994854906599 \\| Test Accuracy: 0.6375935810924229\n","Epoch: 447 \\| Time: 0 \\| Train Loss: 2.9786492443742234 \\| Test Loss: 1.8595002940795964 \\| Test Accuracy: 0.599523281968644\n","Epoch: 448 \\| Time: 0 \\| Train Loss: 3.0207581364573364 \\| Test Loss: 1.8406563009826922 \\| Test Accuracy: 0.6193641521469098\n","Epoch: 449 \\| Time: 0 \\| Train Loss: 2.99386862289762 \\| Test Loss: 1.832794708243767 \\| Test Accuracy: 0.6295363749286601\n","Epoch: 450 \\| Time: 0 \\| Train Loss: 3.002213160672269 \\| Test Loss: 1.8519459006100765 \\| Test Accuracy: 0.6097962198274415\n","Epoch: 451 \\| Time: 0 \\| Train Loss: 3.0268043146720984 \\| Test Loss: 1.8465138970526502 \\| Test Accuracy: 0.614294826602209\n","Epoch: 452 \\| Time: 0 \\| Train Loss: 2.996636785042317 \\| Test Loss: 1.8516126891574123 \\| Test Accuracy: 0.6085204955181791\n","Epoch: 453 \\| Time: 0 \\| Train Loss: 3.0203501841946627 \\| Test Loss: 1.826314330612641 \\| Test Accuracy: 0.6345385570886628\n","Epoch: 454 \\| Time: 0 \\| Train Loss: 3.0124709212181404 \\| Test Loss: 1.8413012119833492 \\| Test Accuracy: 0.6181219995299964\n","Epoch: 455 \\| Time: 0 \\| Train Loss: 2.9859953415090272 \\| Test Loss: 1.8220999890642617 \\| Test Accuracy: 0.6398093127874576\n","Epoch: 456 \\| Time: 0 \\| Train Loss: 3.0216657102783864 \\| Test Loss: 1.8525023496202133 \\| Test Accuracy: 0.6075469164400578\n","Epoch: 457 \\| Time: 0 \\| Train Loss: 3.020230039317875 \\| Test Loss: 1.848633874639421 \\| Test Accuracy: 0.6136569644475778\n","Epoch: 458 \\| Time: 0 \\| Train Loss: 3.0205546074529095 \\| Test Loss: 1.8483205762543904 \\| Test Accuracy: 0.6133212475240877\n","Epoch: 459 \\| Time: 0 \\| Train Loss: 2.989556690776163 \\| Test Loss: 1.815736700536867 \\| Test Accuracy: 0.6460872192567227\n","Epoch: 460 \\| Time: 0 \\| Train Loss: 2.986848840803636 \\| Test Loss: 1.8255541999964244 \\| Test Accuracy: 0.6341356967804747\n","Epoch: 461 \\| Time: 0 \\| Train Loss: 3.0094009754252964 \\| Test Loss: 1.840776219900074 \\| Test Accuracy: 0.6198005841474469\n","Epoch: 462 \\| Time: 0 \\| Train Loss: 3.0307819315941322 \\| Test Loss: 1.8546803053868175 \\| Test Accuracy: 0.6043576056669017\n","Epoch: 463 \\| Time: 0 \\| Train Loss: 3.002724409810736 \\| Test Loss: 1.8616667643125477 \\| Test Accuracy: 0.5976096955047504\n","Epoch: 464 \\| Time: 0 \\| Train Loss: 3.0060259602812067 \\| Test Loss: 1.8292378330435364 \\| Test Accuracy: 0.6307113841608756\n","Epoch: 465 \\| Time: 0 \\| Train Loss: 3.007690142668641 \\| Test Loss: 1.834489712387707 \\| Test Accuracy: 0.6268842112330882\n","Epoch: 466 \\| Time: 0 \\| Train Loss: 3.0047793640097065 \\| Test Loss: 1.8684939463251138 \\| Test Accuracy: 0.590358209957364\n","Epoch: 467 \\| Time: 0 \\| Train Loss: 3.017355019694503 \\| Test Loss: 1.843694121029244 \\| Test Accuracy: 0.6164769866048948\n","Epoch: 468 \\| Time: 0 \\| Train Loss: 3.0044569079381205 \\| Test Loss: 1.8517968148120996 \\| Test Accuracy: 0.6085540672105281\n","Epoch: 469 \\| Time: 0 \\| Train Loss: 3.0146198924384606 \\| Test Loss: 1.855650315468915 \\| Test Accuracy: 0.6041561755128076\n","Epoch: 470 \\| Time: 0 \\| Train Loss: 2.993988502835658 \\| Test Loss: 1.8334321254312735 \\| Test Accuracy: 0.626011347232014\n","Epoch: 471 \\| Time: 0 \\| Train Loss: 2.986921342893556 \\| Test Loss: 1.8232982654939904 \\| Test Accuracy: 0.6385335884781952\n","Epoch: 472 \\| Time: 0 \\| Train Loss: 3.006978437759386 \\| Test Loss: 1.8365857529537872 \\| Test Accuracy: 0.6247356229227515\n","Epoch: 473 \\| Time: 0 \\| Train Loss: 2.990463109568985 \\| Test Loss: 1.8374588018834848 \\| Test Accuracy: 0.6246013361533554\n","Epoch: 474 \\| Time: 0 \\| Train Loss: 2.9735673101112696 \\| Test Loss: 1.8316486127386788 \\| Test Accuracy: 0.6307113841608756\n","Epoch: 475 \\| Time: 0 \\| Train Loss: 2.9928486019183866 \\| Test Loss: 1.8331850188996148 \\| Test Accuracy: 0.6274885016953705\n","Epoch: 476 \\| Time: 0 \\| Train Loss: 3.0230921570566633 \\| Test Loss: 1.861102190140491 \\| Test Accuracy: 0.5979118407358915\n","Epoch: 477 \\| Time: 0 \\| Train Loss: 2.980042392351503 \\| Test Loss: 1.853253624981565 \\| Test Accuracy: 0.6088562124416692\n","Epoch: 478 \\| Time: 0 \\| Train Loss: 2.9780977286822603 \\| Test Loss: 1.8589913532969267 \\| Test Accuracy: 0.599388995199248\n","Epoch: 479 \\| Time: 0 \\| Train Loss: 2.9926978597724476 \\| Test Loss: 1.8185798436275367 \\| Test Accuracy: 0.6432000537147078\n","Epoch: 480 \\| Time: 0 \\| Train Loss: 3.010695754236897 \\| Test Loss: 1.8576408711625783 \\| Test Accuracy: 0.6010340081243496\n","Epoch: 481 \\| Time: 0 \\| Train Loss: 2.9982386672802885 \\| Test Loss: 1.8309485676974186 \\| Test Accuracy: 0.6286970826199348\n","Epoch: 482 \\| Time: 0 \\| Train Loss: 2.9932957066866814 \\| Test Loss: 1.818749314214027 \\| Test Accuracy: 0.6424950481753785\n","Epoch: 483 \\| Time: 0 \\| Train Loss: 3.0233151116613923 \\| Test Loss: 1.8439427527235301 \\| Test Accuracy: 0.6161748413737537\n","Epoch: 484 \\| Time: 0 \\| Train Loss: 2.9855883907132443 \\| Test Loss: 1.825277096212166 \\| Test Accuracy: 0.6366871453989995\n","Epoch: 485 \\| Time: 0 \\| Train Loss: 3.018664632289796 \\| Test Loss: 1.859748225355353 \\| Test Accuracy: 0.599623997045691\n","Epoch: 486 \\| Time: 0 \\| Train Loss: 2.9904599591125978 \\| Test Loss: 1.8516751138949088 \\| Test Accuracy: 0.6094940745963005\n","Epoch: 487 \\| Time: 0 \\| Train Loss: 2.9792682097353658 \\| Test Loss: 1.8233615917197625 \\| Test Accuracy: 0.6377950112465169\n","Epoch: 488 \\| Time: 0 \\| Train Loss: 2.9854921251117204 \\| Test Loss: 1.8222637468141547 \\| Test Accuracy: 0.6406486050961829\n","Epoch: 489 \\| Time: 0 \\| Train Loss: 3.0029655997439093 \\| Test Loss: 1.8515696377201654 \\| Test Accuracy: 0.6107697989055628\n","Epoch: 490 \\| Time: 0 \\| Train Loss: 3.023048367051124 \\| Test Loss: 1.8718658301963316 \\| Test Accuracy: 0.587571759492396\n","Epoch: 491 \\| Time: 0 \\| Train Loss: 3.0047270592300728 \\| Test Loss: 1.8305359464858222 \\| Test Accuracy: 0.6299392352368482\n","Epoch: 492 \\| Time: 0 \\| Train Loss: 3.0107974146167886 \\| Test Loss: 1.8208306905026088 \\| Test Accuracy: 0.6414878974049082\n","Epoch: 493 \\| Time: 0 \\| Train Loss: 2.9865358635512695 \\| Test Loss: 1.8374132368186 \\| Test Accuracy: 0.6236613287675832\n","Epoch: 494 \\| Time: 0 \\| Train Loss: 2.992583689750725 \\| Test Loss: 1.8357683686227757 \\| Test Accuracy: 0.626246349078457\n","Epoch: 495 \\| Time: 0 \\| Train Loss: 3.006357179579056 \\| Test Loss: 1.829523147431566 \\| Test Accuracy: 0.6310471010843657\n","Epoch: 496 \\| Time: 0 \\| Train Loss: 2.9944804786348698 \\| Test Loss: 1.8407656593895778 \\| Test Accuracy: 0.6200020143015409\n","Epoch: 497 \\| Time: 0 \\| Train Loss: 2.987825915814933 \\| Test Loss: 1.8494473972034045 \\| Test Accuracy: 0.611239802598449\n","Epoch: 498 \\| Time: 0 \\| Train Loss: 3.0092105491210037 \\| Test Loss: 1.831240928224228 \\| Test Accuracy: 0.6300063786215463\n","Epoch: 499 \\| Time: 0 \\| Train Loss: 3.0171381846180068 \\| Test Loss: 1.8588505497306202 \\| Test Accuracy: 0.6009668647396516\n","Epoch: 500 \\| Time: 0 \\| Train Loss: 2.990439409547306 \\| Test Loss: 1.831101074750843 \\| Test Accuracy: 0.6310471010843657\n","Epoch: 501 \\| Time: 0 \\| Train Loss: 3.003451383346972 \\| Test Loss: 1.865875085535991 \\| Test Accuracy: 0.5930103736529359\n","Epoch: 502 \\| Time: 0 \\| Train Loss: 3.0195597207553626 \\| Test Loss: 1.8366028331380033 \\| Test Accuracy: 0.6248027663074496\n","Epoch: 503 \\| Time: 0 \\| Train Loss: 2.989982300637995 \\| Test Loss: 1.8751510903559017 \\| Test Accuracy: 0.5838788733340048\n","Epoch: 504 \\| Time: 0 \\| Train Loss: 3.0132800155738124 \\| Test Loss: 1.8712890751883708 \\| Test Accuracy: 0.5879746198005842\n","Epoch: 505 \\| Time: 0 \\| Train Loss: 2.9933709275126983 \\| Test Loss: 1.8565371461180658 \\| Test Accuracy: 0.6037868868969685\n","Epoch: 506 \\| Time: 0 \\| Train Loss: 2.9987901928697056 \\| Test Loss: 1.8383750864364559 \\| Test Accuracy: 0.623292040151744\n","Epoch: 507 \\| Time: 0 \\| Train Loss: 3.0075478100881767 \\| Test Loss: 1.8684556653059603 \\| Test Accuracy: 0.5908282136502501\n","Epoch: 508 \\| Time: 0 \\| Train Loss: 2.9906265133540946 \\| Test Loss: 1.8313250736105595 \\| Test Accuracy: 0.6316178198542989\n","Epoch: 509 \\| Time: 0 \\| Train Loss: 2.9865016384782406 \\| Test Loss: 1.8302357309365989 \\| Test Accuracy: 0.6329606875482593\n","Epoch: 510 \\| Time: 0 \\| Train Loss: 3.023099404630474 \\| Test Loss: 1.8493923521860474 \\| Test Accuracy: 0.610971229059657\n","Epoch: 511 \\| Time: 0 \\| Train Loss: 3.010299882785704 \\| Test Loss: 1.8470083577438485 \\| Test Accuracy: 0.6137576795246248\n","Epoch: 512 \\| Time: 0 \\| Train Loss: 3.007338365942834 \\| Test Loss: 1.8488867625658092 \\| Test Accuracy: 0.6101655084432807\n","Epoch: 513 \\| Time: 0 \\| Train Loss: 2.991620332079467 \\| Test Loss: 1.8490269312019512 \\| Test Accuracy: 0.6128176721388525\n","Epoch: 514 \\| Time: 0 \\| Train Loss: 2.9949331733581346 \\| Test Loss: 1.8268345128824783 \\| Test Accuracy: 0.6354449927820861\n","Epoch: 515 \\| Time: 0 \\| Train Loss: 2.987357083704077 \\| Test Loss: 1.8641308955368565 \\| Test Accuracy: 0.596434686272535\n","Epoch: 516 \\| Time: 0 \\| Train Loss: 2.99985157953165 \\| Test Loss: 1.8273925105901234 \\| Test Accuracy: 0.6342028401651727\n","Epoch: 517 \\| Time: 0 \\| Train Loss: 2.9905250085976 \\| Test Loss: 1.831255492734295 \\| Test Accuracy: 0.6304428106220834\n","Epoch: 518 \\| Time: 0 \\| Train Loss: 3.012383408494072 \\| Test Loss: 1.8454439542835874 \\| Test Accuracy: 0.614496256756303\n","Epoch: 519 \\| Time: 0 \\| Train Loss: 2.9918306015226026 \\| Test Loss: 1.841971211678992 \\| Test Accuracy: 0.6193977238392587\n","Epoch: 520 \\| Time: 0 \\| Train Loss: 3.0209683332745954 \\| Test Loss: 1.8260158116213754 \\| Test Accuracy: 0.6355121361667841\n","Epoch: 521 \\| Time: 0 \\| Train Loss: 3.0031941080394082 \\| Test Loss: 1.8166469734625754 \\| Test Accuracy: 0.6459193607949777\n","Epoch: 522 \\| Time: 0 \\| Train Loss: 2.9932841471235725 \\| Test Loss: 1.8292275728585894 \\| Test Accuracy: 0.6329942592406084\n","Epoch: 523 \\| Time: 0 \\| Train Loss: 3.0200549066201914 \\| Test Loss: 1.8275262854129972 \\| Test Accuracy: 0.6322892537012791\n","Epoch: 524 \\| Time: 0 \\| Train Loss: 3.005606011603415 \\| Test Loss: 1.8446384811606018 \\| Test Accuracy: 0.6165777016819418\n","Epoch: 525 \\| Time: 0 \\| Train Loss: 3.0097272496183276 \\| Test Loss: 1.838173536272008 \\| Test Accuracy: 0.6218484573807366\n","Epoch: 526 \\| Time: 0 \\| Train Loss: 3.006738315464581 \\| Test Loss: 1.8756223185379617 \\| Test Accuracy: 0.5832745828717225\n","Epoch: 527 \\| Time: 0 \\| Train Loss: 2.953564187838076 \\| Test Loss: 1.8489192586088385 \\| Test Accuracy: 0.6110383724443549\n","Epoch: 528 \\| Time: 0 \\| Train Loss: 3.0171558710886495 \\| Test Loss: 1.864955163308991 \\| Test Accuracy: 0.5928089434988417\n","Epoch: 529 \\| Time: 0 \\| Train Loss: 3.0039889443231553 \\| Test Loss: 1.8239113716608466 \\| Test Accuracy: 0.6368550038607447\n","Epoch: 530 \\| Time: 0 \\| Train Loss: 2.9728072360886824 \\| Test Loss: 1.8500434057906974 \\| Test Accuracy: 0.6096619330580455\n","Epoch: 531 \\| Time: 0 \\| Train Loss: 2.9891819656564445 \\| Test Loss: 1.839970464358514 \\| Test Accuracy: 0.6196662973780508\n","Epoch: 532 \\| Time: 0 \\| Train Loss: 2.9858465113432806 \\| Test Loss: 1.839032261156729 \\| Test Accuracy: 0.6228556081512069\n","Epoch: 533 \\| Time: 0 \\| Train Loss: 3.0259780329432 \\| Test Loss: 1.8905663623318651 \\| Test Accuracy: 0.5685701816228556\n","Epoch: 534 \\| Time: 0 \\| Train Loss: 3.0199156666874765 \\| Test Loss: 1.8282711562168956 \\| Test Accuracy: 0.6329942592406084\n","Epoch: 535 \\| Time: 0 \\| Train Loss: 3.036288779698011 \\| Test Loss: 1.8331741360635716 \\| Test Accuracy: 0.6268170678483902\n","Epoch: 536 \\| Time: 0 \\| Train Loss: 3.0396329228742376 \\| Test Loss: 1.849060404965806 \\| Test Accuracy: 0.6121798099842213\n","Epoch: 537 \\| Time: 0 \\| Train Loss: 3.017447109324871 \\| Test Loss: 1.8321942766336925 \\| Test Accuracy: 0.6286635109275859\n","Epoch: 538 \\| Time: 0 \\| Train Loss: 3.0042057502968755 \\| Test Loss: 1.8333846325526422 \\| Test Accuracy: 0.6264477792325511\n","Epoch: 539 \\| Time: 0 \\| Train Loss: 2.9985135767038935 \\| Test Loss: 1.8379588188531573 \\| Test Accuracy: 0.6224863195353678\n","Epoch: 540 \\| Time: 0 \\| Train Loss: 3.0030961428587997 \\| Test Loss: 1.916164005263169 \\| Test Accuracy: 0.5426864068217678\n","Epoch: 541 \\| Time: 0 \\| Train Loss: 2.984420641256201 \\| Test Loss: 1.8392099029516458 \\| Test Accuracy: 0.6213784536878504\n","Epoch: 542 \\| Time: 0 \\| Train Loss: 3.0070203264496427 \\| Test Loss: 1.829324860429559 \\| Test Accuracy: 0.6300735220062443\n","Epoch: 543 \\| Time: 0 \\| Train Loss: 3.0137398166068095 \\| Test Loss: 1.8302157242410684 \\| Test Accuracy: 0.6303420955450364\n","Epoch: 544 \\| Time: 0 \\| Train Loss: 2.995552926623722 \\| Test Loss: 1.8182243558981899 \\| Test Accuracy: 0.6431664820223587\n","Epoch: 545 \\| Time: 0 \\| Train Loss: 3.016617633880162 \\| Test Loss: 1.8414271026210212 \\| Test Accuracy: 0.6190955786081176\n","Epoch: 546 \\| Time: 0 \\| Train Loss: 2.996292397841649 \\| Test Loss: 1.8423893374946496 \\| Test Accuracy: 0.6183905730687884\n","Epoch: 547 \\| Time: 0 \\| Train Loss: 3.0499413494630714 \\| Test Loss: 1.8344044572805642 \\| Test Accuracy: 0.6257427736932218\n","Epoch: 548 \\| Time: 0 \\| Train Loss: 2.9961321704808084 \\| Test Loss: 1.8517197314250111 \\| Test Accuracy: 0.6075469164400578\n","Epoch: 549 \\| Time: 0 \\| Train Loss: 3.01414175953396 \\| Test Loss: 1.8568085907866514 \\| Test Accuracy: 0.6049283244368349\n","Epoch: 550 \\| Time: 0 \\| Train Loss: 3.0020410871563414 \\| Test Loss: 1.8488737503346455 \\| Test Accuracy: 0.6119783798301273\n","Epoch: 551 \\| Time: 0 \\| Train Loss: 3.0397335990578016 \\| Test Loss: 1.8364904760802765 \\| Test Accuracy: 0.6245006210763084\n","Epoch: 552 \\| Time: 0 \\| Train Loss: 3.0195618514473948 \\| Test Loss: 1.8508337439385607 \\| Test Accuracy: 0.6072111995165677\n","Epoch: 553 \\| Time: 0 \\| Train Loss: 2.9814307600470116 \\| Test Loss: 1.8300853563480624 \\| Test Accuracy: 0.6320878235471851\n","Epoch: 554 \\| Time: 0 \\| Train Loss: 2.986491554762726 \\| Test Loss: 1.851833261645403 \\| Test Accuracy: 0.6086547822875751\n","Epoch: 555 \\| Time: 0 \\| Train Loss: 3.001790338361943 \\| Test Loss: 1.835347482063228 \\| Test Accuracy: 0.6265149226172492\n","Epoch: 556 \\| Time: 0 \\| Train Loss: 3.0012736943455365 \\| Test Loss: 1.8231699190426283 \\| Test Accuracy: 0.6374928660153758\n","Epoch: 557 \\| Time: 0 \\| Train Loss: 2.9910334897452255 \\| Test Loss: 1.8300616956064117 \\| Test Accuracy: 0.6314499613925538\n","Epoch: 558 \\| Time: 0 \\| Train Loss: 3.0524299235633476 \\| Test Loss: 1.8434762161688742 \\| Test Accuracy: 0.61698056199013\n","Epoch: 559 \\| Time: 0 \\| Train Loss: 2.999174108704385 \\| Test Loss: 1.8437828071127633 \\| Test Accuracy: 0.6177191392218082\n","Epoch: 560 \\| Time: 0 \\| Train Loss: 3.013956780319223 \\| Test Loss: 1.8343896246775024 \\| Test Accuracy: 0.6272534998489274\n","Epoch: 561 \\| Time: 0 \\| Train Loss: 2.9962672268732624 \\| Test Loss: 1.8336948729380005 \\| Test Accuracy: 0.6282270789270488\n","Epoch: 562 \\| Time: 0 \\| Train Loss: 3.004485600310299 \\| Test Loss: 1.854912308664281 \\| Test Accuracy: 0.6046597508980428\n","Epoch: 563 \\| Time: 0 \\| Train Loss: 2.9928987162436096 \\| Test Loss: 1.8284284715488746 \\| Test Accuracy: 0.6314835330849028\n","Epoch: 564 \\| Time: 0 \\| Train Loss: 2.9932818531422347 \\| Test Loss: 1.8338985617068704 \\| Test Accuracy: 0.6277570752341626\n","Epoch: 565 \\| Time: 0 \\| Train Loss: 3.019059503055654 \\| Test Loss: 1.8930056780704614 \\| Test Accuracy: 0.566522308389566\n","Epoch: 566 \\| Time: 0 \\| Train Loss: 2.9946070863983136 \\| Test Loss: 1.8368208674402196 \\| Test Accuracy: 0.6237956155369793\n","Epoch: 567 \\| Time: 0 \\| Train Loss: 2.992456916135303 \\| Test Loss: 1.8589793116237985 \\| Test Accuracy: 0.6006311478161614\n","Epoch: 568 \\| Time: 0 \\| Train Loss: 3.0333522434996043 \\| Test Loss: 1.8474631156020922 \\| Test Accuracy: 0.6120119515224762\n","Epoch: 569 \\| Time: 0 \\| Train Loss: 2.9969659450303223 \\| Test Loss: 1.8356288471958668 \\| Test Accuracy: 0.6246684795380535\n","Epoch: 570 \\| Time: 0 \\| Train Loss: 2.9891073021797037 \\| Test Loss: 1.8338322813418801 \\| Test Accuracy: 0.6279920770806057\n","Epoch: 571 \\| Time: 0 \\| Train Loss: 3.013605552169819 \\| Test Loss: 1.834943822013462 \\| Test Accuracy: 0.6257763453855709\n","Epoch: 572 \\| Time: 0 \\| Train Loss: 2.9811441982398557 \\| Test Loss: 1.8451535466402897 \\| Test Accuracy: 0.6146976869103972\n","Epoch: 573 \\| Time: 0 \\| Train Loss: 2.98832463252422 \\| Test Loss: 1.8650431054856134 \\| Test Accuracy: 0.5946553865780374\n","Epoch: 574 \\| Time: 0 \\| Train Loss: 3.016186099334319 \\| Test Loss: 1.847387194121856 \\| Test Accuracy: 0.6120790949071743\n","Epoch: 575 \\| Time: 0 \\| Train Loss: 2.9901469492636745 \\| Test Loss: 1.8326019253341936 \\| Test Accuracy: 0.6293349447745661\n","Epoch: 576 \\| Time: 0 \\| Train Loss: 3.0115677650918067 \\| Test Loss: 1.8291363424497613 \\| Test Accuracy: 0.6327592573941653\n","Epoch: 577 \\| Time: 0 \\| Train Loss: 2.9953688152211724 \\| Test Loss: 1.8193647574969116 \\| Test Accuracy: 0.6427636217141706\n","Epoch: 578 \\| Time: 0 \\| Train Loss: 2.9877111559022134 \\| Test Loss: 1.8304758721666787 \\| Test Accuracy: 0.6307113841608756\n","Epoch: 579 \\| Time: 0 \\| Train Loss: 2.9741298934095926 \\| Test Loss: 1.8479072326242667 \\| Test Accuracy: 0.6131869607546916\n","Epoch: 580 \\| Time: 0 \\| Train Loss: 2.9815734186261706 \\| Test Loss: 1.8111672815846782 \\| Test Accuracy: 0.6507872561855843\n","Epoch: 581 \\| Time: 0 \\| Train Loss: 2.960587963570564 \\| Test Loss: 1.8163785970262192 \\| Test Accuracy: 0.6461879343337698\n","Epoch: 582 \\| Time: 0 \\| Train Loss: 2.970750083495214 \\| Test Loss: 1.833669371870966 \\| Test Accuracy: 0.626011347232014\n","Epoch: 583 \\| Time: 0 \\| Train Loss: 2.996101261831191 \\| Test Loss: 1.8450658331613172 \\| Test Accuracy: 0.6147648302950952\n","Epoch: 584 \\| Time: 0 \\| Train Loss: 2.9727033235457445 \\| Test Loss: 1.8377947147312081 \\| Test Accuracy: 0.623359183536442\n","Epoch: 585 \\| Time: 0 \\| Train Loss: 2.9983361754586406 \\| Test Loss: 1.819946544876426 \\| Test Accuracy: 0.640883606942626\n","Epoch: 586 \\| Time: 0 \\| Train Loss: 2.983953413684536 \\| Test Loss: 1.8749082728005275 \\| Test Accuracy: 0.5848524524121261\n","Epoch: 587 \\| Time: 0 \\| Train Loss: 3.0080401729119504 \\| Test Loss: 1.8212091303690308 \\| Test Accuracy: 0.6415550407896062\n","Epoch: 588 \\| Time: 0 \\| Train Loss: 2.974358045460965 \\| Test Loss: 1.8292244381147393 \\| Test Accuracy: 0.6323899687783261\n","Epoch: 589 \\| Time: 0 \\| Train Loss: 3.040543568066398 \\| Test Loss: 1.8650377206024693 \\| Test Accuracy: 0.593245375499379\n","Epoch: 590 \\| Time: 0 \\| Train Loss: 2.9826496654914627 \\| Test Loss: 1.8296520218828718 \\| Test Accuracy: 0.6318863933930909\n","Epoch: 591 \\| Time: 0 \\| Train Loss: 2.978179373788043 \\| Test Loss: 1.8576075595847528 \\| Test Accuracy: 0.6011347232013966\n","Epoch: 592 \\| Time: 0 \\| Train Loss: 2.988075416327021 \\| Test Loss: 1.8325681430587442 \\| Test Accuracy: 0.6293013730822171\n","Epoch: 593 \\| Time: 0 \\| Train Loss: 3.0109061098858625 \\| Test Loss: 1.8221459255709669 \\| Test Accuracy: 0.6387350186322892\n","Epoch: 594 \\| Time: 0 \\| Train Loss: 3.015154303687364 \\| Test Loss: 1.851297119656346 \\| Test Accuracy: 0.6115755195219391\n","Epoch: 595 \\| Time: 0 \\| Train Loss: 3.008364853666473 \\| Test Loss: 1.8329022529299168 \\| Test Accuracy: 0.6284620807734917\n","Epoch: 596 \\| Time: 0 \\| Train Loss: 3.0133345574560537 \\| Test Loss: 1.8404218690078147 \\| Test Accuracy: 0.6211098801490583\n","Epoch: 597 \\| Time: 0 \\| Train Loss: 2.9932153819496983 \\| Test Loss: 1.8287231379824136 \\| Test Accuracy: 0.6329942592406084\n","Epoch: 598 \\| Time: 0 \\| Train Loss: 3.011595534154877 \\| Test Loss: 1.828295807470068 \\| Test Accuracy: 0.6335985497028905\n","Epoch: 599 \\| Time: 0 \\| Train Loss: 2.9758799030953385 \\| Test Loss: 1.8153191554188217 \\| Test Accuracy: 0.6458857891026286\n","Epoch: 600 \\| Time: 0 \\| Train Loss: 2.996029184279756 \\| Test Loss: 1.8335536247670907 \\| Test Accuracy: 0.6274213583106725\n","Epoch: 601 \\| Time: 0 \\| Train Loss: 3.018672943518008 \\| Test Loss: 1.8441677727924397 \\| Test Accuracy: 0.6166112733742908\n","Epoch: 602 \\| Time: 0 \\| Train Loss: 3.0070173225718473 \\| Test Loss: 1.8184073559716023 \\| Test Accuracy: 0.6442743478698761\n","Epoch: 603 \\| Time: 0 \\| Train Loss: 2.985615660965202 \\| Test Loss: 1.8503807839405895 \\| Test Accuracy: 0.6098297915197906\n","Epoch: 604 \\| Time: 0 \\| Train Loss: 2.9932454276430014 \\| Test Loss: 1.8322833674148429 \\| Test Accuracy: 0.628965656158727\n","Epoch: 605 \\| Time: 0 \\| Train Loss: 3.012572695525742 \\| Test Loss: 1.8181021310740786 \\| Test Accuracy: 0.6444757780239702\n","Epoch: 606 \\| Time: 0 \\| Train Loss: 3.0282027587456466 \\| Test Loss: 1.8437812604617663 \\| Test Accuracy: 0.6169469902977809\n","Epoch: 607 \\| Time: 0 \\| Train Loss: 3.021080496888442 \\| Test Loss: 1.8676779382730246 \\| Test Accuracy: 0.5900224930338739\n","Epoch: 608 \\| Time: 0 \\| Train Loss: 3.03103947901199 \\| Test Loss: 1.8312297460858913 \\| Test Accuracy: 0.6303756672373855\n","Epoch: 609 \\| Time: 0 \\| Train Loss: 3.019066563226322 \\| Test Loss: 1.8683659902458027 \\| Test Accuracy: 0.5917682210360224\n","Epoch: 610 \\| Time: 0 \\| Train Loss: 3.024901647071706 \\| Test Loss: 1.8369458294733398 \\| Test Accuracy: 0.6239634739987243\n","Epoch: 611 \\| Time: 0 \\| Train Loss: 2.9549050632199627 \\| Test Loss: 1.8353937429419915 \\| Test Accuracy: 0.6256420586161748\n","Epoch: 612 \\| Time: 0 \\| Train Loss: 3.003818299858895 \\| Test Loss: 1.8520971843613063 \\| Test Accuracy: 0.608352637056434\n","Epoch: 613 \\| Time: 0 \\| Train Loss: 3.002481406940473 \\| Test Loss: 1.8466919655451959 \\| Test Accuracy: 0.6128848155235506\n","Epoch: 614 \\| Time: 0 \\| Train Loss: 2.998897408212124 \\| Test Loss: 1.8494255317638872 \\| Test Accuracy: 0.6117769496760331\n","Epoch: 615 \\| Time: 0 \\| Train Loss: 3.014690639479482 \\| Test Loss: 1.8785679555246246 \\| Test Accuracy: 0.5802531305603116\n","Epoch: 616 \\| Time: 0 \\| Train Loss: 2.971722423628619 \\| Test Loss: 1.8434745610527725 \\| Test Accuracy: 0.617349850605969\n","Epoch: 617 \\| Time: 0 \\| Train Loss: 3.000428258029281 \\| Test Loss: 1.8269619389153346 \\| Test Accuracy: 0.6341692684728237\n","Epoch: 618 \\| Time: 0 \\| Train Loss: 2.984418908233532 \\| Test Loss: 1.814663737628593 \\| Test Accuracy: 0.6478665189512204\n","Epoch: 619 \\| Time: 0 \\| Train Loss: 2.9873761826578966 \\| Test Loss: 1.8810329201907048 \\| Test Accuracy: 0.5797159834827273\n","Epoch: 620 \\| Time: 0 \\| Train Loss: 3.0170541987893804 \\| Test Loss: 1.848726538117863 \\| Test Accuracy: 0.6107362272132139\n","Epoch: 621 \\| Time: 0 \\| Train Loss: 3.0092145088710813 \\| Test Loss: 1.8252264061710866 \\| Test Accuracy: 0.6358814247826233\n","Epoch: 622 \\| Time: 0 \\| Train Loss: 3.010575439156347 \\| Test Loss: 1.8389686141402937 \\| Test Accuracy: 0.6219827441501327\n","Epoch: 623 \\| Time: 0 \\| Train Loss: 3.0090690911852027 \\| Test Loss: 1.8338051359029286 \\| Test Accuracy: 0.626313492463155\n","Epoch: 624 \\| Time: 0 \\| Train Loss: 2.9895927343524105 \\| Test Loss: 1.8312681672910764 \\| Test Accuracy: 0.6302078087756404\n","Epoch: 625 \\| Time: 0 \\| Train Loss: 2.9971743693002137 \\| Test Loss: 1.8348849643453509 \\| Test Accuracy: 0.6251720549232886\n","Epoch: 626 \\| Time: 0 \\| Train Loss: 3.024098545728912 \\| Test Loss: 1.8946961410055856 \\| Test Accuracy: 0.5627287071541276\n","Epoch: 627 \\| Time: 0 \\| Train Loss: 2.998299553087716 \\| Test Loss: 1.8284458060121331 \\| Test Accuracy: 0.6318192500083929\n","Epoch: 628 \\| Time: 0 \\| Train Loss: 3.018243448522493 \\| Test Loss: 1.8346043610265839 \\| Test Accuracy: 0.6257763453855709\n","Epoch: 629 \\| Time: 0 \\| Train Loss: 3.0079102211998867 \\| Test Loss: 1.8495230848697122 \\| Test Accuracy: 0.6099305065968376\n","Epoch: 630 \\| Time: 0 \\| Train Loss: 3.0045872970958736 \\| Test Loss: 1.8580311168416888 \\| Test Accuracy: 0.6008661496626045\n","Epoch: 631 \\| Time: 0 \\| Train Loss: 3.0115724277350204 \\| Test Loss: 1.8563113534910995 \\| Test Accuracy: 0.6043911773592506\n","Epoch: 632 \\| Time: 0 \\| Train Loss: 2.996486162960176 \\| Test Loss: 1.8590417563147812 \\| Test Accuracy: 0.5998589988921341\n","Epoch: 633 \\| Time: 0 \\| Train Loss: 2.999702907125025 \\| Test Loss: 1.8275681698271133 \\| Test Accuracy: 0.6345721287810119\n","Epoch: 634 \\| Time: 0 \\| Train Loss: 3.0070164544613096 \\| Test Loss: 1.8386143584108148 \\| Test Accuracy: 0.6217477423036896\n","Epoch: 635 \\| Time: 0 \\| Train Loss: 3.0317306766186265 \\| Test Loss: 1.8394468992053183 \\| Test Accuracy: 0.6214120253801995\n","Epoch: 636 \\| Time: 0 \\| Train Loss: 2.9987435228959916 \\| Test Loss: 1.85011792029434 \\| Test Accuracy: 0.610971229059657\n","Epoch: 637 \\| Time: 0 \\| Train Loss: 2.968110570826222 \\| Test Loss: 1.8161522277946636 \\| Test Accuracy: 0.6462215060261188\n","Epoch: 638 \\| Time: 0 \\| Train Loss: 3.0161651482278122 \\| Test Loss: 1.863683017026713 \\| Test Accuracy: 0.5960989693490449\n","Epoch: 639 \\| Time: 0 \\| Train Loss: 3.0389351109551823 \\| Test Loss: 1.8791184077447065 \\| Test Accuracy: 0.5804209890220566\n","Epoch: 640 \\| Time: 0 \\| Train Loss: 3.010416121789731 \\| Test Loss: 1.8499280331984098 \\| Test Accuracy: 0.6117433779836842\n","Epoch: 641 \\| Time: 0 \\| Train Loss: 3.0017070039025535 \\| Test Loss: 1.8425715511960532 \\| Test Accuracy: 0.6180548561452983\n","Epoch: 642 \\| Time: 0 \\| Train Loss: 3.0194948021119505 \\| Test Loss: 1.8335798651363717 \\| Test Accuracy: 0.6275556450800684\n","Epoch: 643 \\| Time: 0 \\| Train Loss: 3.0017974288386515 \\| Test Loss: 1.8240385009495486 \\| Test Accuracy: 0.6356128512438312\n","Epoch: 644 \\| Time: 0 \\| Train Loss: 2.9907755480521914 \\| Test Loss: 1.8714077856407656 \\| Test Accuracy: 0.587336757645953\n","Epoch: 645 \\| Time: 0 \\| Train Loss: 2.9999833168694576 \\| Test Loss: 1.8330962228161072 \\| Test Accuracy: 0.6295699466210092\n","Epoch: 646 \\| Time: 0 \\| Train Loss: 3.0184629156869422 \\| Test Loss: 1.8486121064091956 \\| Test Accuracy: 0.6128176721388525\n","Epoch: 647 \\| Time: 0 \\| Train Loss: 3.011973084416399 \\| Test Loss: 1.8376082729372343 \\| Test Accuracy: 0.6238627589216772\n","Epoch: 648 \\| Time: 0 \\| Train Loss: 2.995744140234289 \\| Test Loss: 1.830902265376799 \\| Test Accuracy: 0.629267801389868\n","Epoch: 649 \\| Time: 0 \\| Train Loss: 3.0001339128635154 \\| Test Loss: 1.8378784247222377 \\| Test Accuracy: 0.6225870346124148\n","Epoch: 650 \\| Time: 0 \\| Train Loss: 3.0098145246317882 \\| Test Loss: 1.8411177637239382 \\| Test Accuracy: 0.6192970087622117\n","Epoch: 651 \\| Time: 0 \\| Train Loss: 3.015943021417984 \\| Test Loss: 1.8573449732407992 \\| Test Accuracy: 0.6021754456642159\n","Epoch: 652 \\| Time: 0 \\| Train Loss: 3.0122495969875085 \\| Test Loss: 1.8571249574039115 \\| Test Accuracy: 0.6030818813576393\n","Epoch: 653 \\| Time: 0 \\| Train Loss: 3.001688544041006 \\| Test Loss: 1.8396327679760978 \\| Test Accuracy: 0.6215463121495954\n","Epoch: 654 \\| Time: 0 \\| Train Loss: 3.0189463450775658 \\| Test Loss: 1.833049604821103 \\| Test Accuracy: 0.6283949373887938\n","Epoch: 655 \\| Time: 0 \\| Train Loss: 3.0169915086980383 \\| Test Loss: 1.9009043914565713 \\| Test Accuracy: 0.557860811763521\n","Epoch: 656 \\| Time: 0 \\| Train Loss: 3.0196138033715627 \\| Test Loss: 1.8605511648972146 \\| Test Accuracy: 0.5987847047369658\n","Epoch: 657 \\| Time: 0 \\| Train Loss: 3.022769725583144 \\| Test Loss: 1.8819899589718667 \\| Test Accuracy: 0.5762245274784302\n","Epoch: 658 \\| Time: 0 \\| Train Loss: 2.9893380006027996 \\| Test Loss: 1.8628066696322527 \\| Test Accuracy: 0.5969718333501192\n","Epoch: 659 \\| Time: 0 \\| Train Loss: 2.999117489280723 \\| Test Loss: 1.8463914788332108 \\| Test Accuracy: 0.6169469902977809\n","Epoch: 660 \\| Time: 0 \\| Train Loss: 2.989158310943686 \\| Test Loss: 1.8581999555677815 \\| Test Accuracy: 0.6025111625877061\n","Epoch: 661 \\| Time: 0 \\| Train Loss: 3.0281137315180304 \\| Test Loss: 1.8549847572146567 \\| Test Accuracy: 0.6044247490515997\n","Epoch: 662 \\| Time: 0 \\| Train Loss: 2.99856364820158 \\| Test Loss: 1.8350380347010404 \\| Test Accuracy: 0.6267163527713432\n","Epoch: 663 \\| Time: 0 \\| Train Loss: 2.9763412443743977 \\| Test Loss: 1.8323176173181492 \\| Test Accuracy: 0.628898512774029\n","Epoch: 664 \\| Time: 0 \\| Train Loss: 2.9946288560782706 \\| Test Loss: 1.85130474598111 \\| Test Accuracy: 0.608218350287038\n","Epoch: 665 \\| Time: 0 \\| Train Loss: 3.025234999391547 \\| Test Loss: 1.8366283904841019 \\| Test Accuracy: 0.6226541779971129\n","Epoch: 666 \\| Time: 0 \\| Train Loss: 3.0168240595269795 \\| Test Loss: 1.822684497280694 \\| Test Accuracy: 0.6384328734011482\n","Epoch: 667 \\| Time: 0 \\| Train Loss: 2.991478915004996 \\| Test Loss: 1.9162895382729723 \\| Test Accuracy: 0.5431899822070031\n","Epoch: 668 \\| Time: 0 \\| Train Loss: 2.999032297011489 \\| Test Loss: 1.8224516462358793 \\| Test Accuracy: 0.6389700204787323\n","Epoch: 669 \\| Time: 0 \\| Train Loss: 2.983507984798411 \\| Test Loss: 1.8417157743110166 \\| Test Accuracy: 0.6191291503004667\n","Epoch: 670 \\| Time: 0 \\| Train Loss: 2.9717765715703477 \\| Test Loss: 1.8174922241161822 \\| Test Accuracy: 0.643770772484641\n","Epoch: 671 \\| Time: 0 \\| Train Loss: 2.9944069483714806 \\| Test Loss: 1.8537202014431933 \\| Test Accuracy: 0.6066740524389834\n","Epoch: 672 \\| Time: 0 \\| Train Loss: 3.020643430421554 \\| Test Loss: 1.8662113186627498 \\| Test Accuracy: 0.5940510961157552\n","Epoch: 673 \\| Time: 0 \\| Train Loss: 3.0067376209889574 \\| Test Loss: 1.8307516492999163 \\| Test Accuracy: 0.6288649410816799\n","Epoch: 674 \\| Time: 0 \\| Train Loss: 3.0163831947031308 \\| Test Loss: 1.827988066898395 \\| Test Accuracy: 0.6338335515493336\n","Epoch: 675 \\| Time: 0 \\| Train Loss: 2.959694141416031 \\| Test Loss: 1.8356793637951045 \\| Test Accuracy: 0.6247356229227515\n","Epoch: 676 \\| Time: 0 \\| Train Loss: 3.0090818736472764 \\| Test Loss: 1.8286300841319203 \\| Test Accuracy: 0.6335314063181925\n","Epoch: 677 \\| Time: 0 \\| Train Loss: 3.0172647508105657 \\| Test Loss: 1.846322570747572 \\| Test Accuracy: 0.6130191022929465\n","Epoch: 678 \\| Time: 0 \\| Train Loss: 3.0129763096264686 \\| Test Loss: 1.8258214948515012 \\| Test Accuracy: 0.6357135663208783\n","Epoch: 679 \\| Time: 0 \\| Train Loss: 3.0043860255839023 \\| Test Loss: 1.8491663047684108 \\| Test Accuracy: 0.6100983650585826\n","Epoch: 680 \\| Time: 0 \\| Train Loss: 3.039102439674471 \\| Test Loss: 1.888345099314088 \\| Test Accuracy: 0.5700809077785611\n","Epoch: 681 \\| Time: 0 \\| Train Loss: 3.018021133192233 \\| Test Loss: 1.8537041440030535 \\| Test Accuracy: 0.6084533521334811\n","Epoch: 682 \\| Time: 0 \\| Train Loss: 2.979677399288889 \\| Test Loss: 1.8438009115759395 \\| Test Accuracy: 0.6162419847584517\n","Epoch: 683 \\| Time: 0 \\| Train Loss: 3.0173640768254426 \\| Test Loss: 1.8235234047721895 \\| Test Accuracy: 0.6382650149394031\n","Epoch: 684 \\| Time: 0 \\| Train Loss: 3.005735334470234 \\| Test Loss: 1.8292835930386326 \\| Test Accuracy: 0.632322825393628\n","Epoch: 685 \\| Time: 0 \\| Train Loss: 2.99835768796324 \\| Test Loss: 1.8526518109530339 \\| Test Accuracy: 0.6079833484405949\n","Epoch: 686 \\| Time: 0 \\| Train Loss: 2.997641702298753 \\| Test Loss: 1.8454686382809422 \\| Test Accuracy: 0.6154026924497263\n","Epoch: 687 \\| Time: 0 \\| Train Loss: 3.0257529721023766 \\| Test Loss: 1.8633479263649477 \\| Test Accuracy: 0.5958975391949508\n","Epoch: 688 \\| Time: 0 \\| Train Loss: 3.002900764939848 \\| Test Loss: 1.8488183461545362 \\| Test Accuracy: 0.6108705139826098\n","Epoch: 689 \\| Time: 0 \\| Train Loss: 3.001537961264033 \\| Test Loss: 1.8378611521659491 \\| Test Accuracy: 0.6236277570752342\n","Epoch: 690 \\| Time: 0 \\| Train Loss: 2.998321097713032 \\| Test Loss: 1.843307268466049 \\| Test Accuracy: 0.6189948635310706\n","Epoch: 691 \\| Time: 0 \\| Train Loss: 2.9873903201958902 \\| Test Loss: 1.8204057283155908 \\| Test Accuracy: 0.6412193238661161\n","Epoch: 692 \\| Time: 0 \\| Train Loss: 3.0223478614097607 \\| Test Loss: 1.8449262963855726 \\| Test Accuracy: 0.6151341189109343\n","Epoch: 693 \\| Time: 0 \\| Train Loss: 2.992751135147766 \\| Test Loss: 1.8428494551662724 \\| Test Accuracy: 0.6183570013764393\n","Epoch: 694 \\| Time: 0 \\| Train Loss: 2.9890677495430746 \\| Test Loss: 1.829367922099363 \\| Test Accuracy: 0.6331956893947024\n","Epoch: 695 \\| Time: 0 \\| Train Loss: 2.978399641766475 \\| Test Loss: 1.8409236705354355 \\| Test Accuracy: 0.6191291503004667\n","Epoch: 696 \\| Time: 0 \\| Train Loss: 2.9954732338208547 \\| Test Loss: 1.845419891914073 \\| Test Accuracy: 0.6145634001410011\n","Epoch: 697 \\| Time: 0 \\| Train Loss: 3.0000371857760997 \\| Test Loss: 1.8518949967085547 \\| Test Accuracy: 0.6092590727498574\n","Epoch: 698 \\| Time: 0 \\| Train Loss: 3.0105368654622473 \\| Test Loss: 1.8535540799726233 \\| Test Accuracy: 0.6061369053613993\n","Epoch: 699 \\| Time: 0 \\| Train Loss: 3.005424730361036 \\| Test Loss: 1.8353784437343286 \\| Test Accuracy: 0.6261792056937591\n","Epoch: 700 \\| Time: 0 \\| Train Loss: 3.0054171780865975 \\| Test Loss: 1.8483220113705157 \\| Test Accuracy: 0.611441232752543\n","Epoch: 701 \\| Time: 0 \\| Train Loss: 3.022956997390134 \\| Test Loss: 1.8415832468368465 \\| Test Accuracy: 0.6183234296840904\n","Epoch: 702 \\| Time: 0 \\| Train Loss: 2.9818067583948227 \\| Test Loss: 1.8497950048405725 \\| Test Accuracy: 0.6100312216738846\n","Epoch: 703 \\| Time: 0 \\| Train Loss: 3.0256130684338642 \\| Test Loss: 1.8491225488196115 \\| Test Accuracy: 0.6117433779836842\n","Epoch: 704 \\| Time: 0 \\| Train Loss: 3.0159773569063324 \\| Test Loss: 1.8386880548215219 \\| Test Accuracy: 0.6219156007654346\n","Epoch: 705 \\| Time: 0 \\| Train Loss: 3.0047345716893017 \\| Test Loss: 1.8155877712970128 \\| Test Accuracy: 0.6454157854097425\n","Epoch: 706 \\| Time: 0 \\| Train Loss: 3.0227928522055088 \\| Test Loss: 1.8520035124643677 \\| Test Accuracy: 0.6093933595192533\n","Epoch: 707 \\| Time: 0 \\| Train Loss: 2.9956345578136303 \\| Test Loss: 1.8835513914091904 \\| Test Accuracy: 0.5750159465538658\n","Epoch: 708 \\| Time: 0 \\| Train Loss: 3.0152761670436368 \\| Test Loss: 1.8374529688143424 \\| Test Accuracy: 0.6246684795380535\n","Epoch: 709 \\| Time: 0 \\| Train Loss: 3.0200798174390524 \\| Test Loss: 1.8500995175521262 \\| Test Accuracy: 0.6094269312116024\n","Epoch: 710 \\| Time: 0 \\| Train Loss: 2.9898820845459864 \\| Test Loss: 1.8434957086784134 \\| Test Accuracy: 0.6176184241447611\n","Epoch: 711 \\| Time: 0 \\| Train Loss: 2.988816798320499 \\| Test Loss: 1.8455867828729327 \\| Test Accuracy: 0.6140598247557659\n","Epoch: 712 \\| Time: 0 \\| Train Loss: 3.0116862235271578 \\| Test Loss: 1.8787741001071847 \\| Test Accuracy: 0.5811259945613858\n","Epoch: 713 \\| Time: 0 \\| Train Loss: 2.9914275297680684 \\| Test Loss: 1.8889157859040944 \\| Test Accuracy: 0.5704166247020512\n","Epoch: 714 \\| Time: 0 \\| Train Loss: 3.0330603915415613 \\| Test Loss: 1.8217151917101488 \\| Test Accuracy: 0.6393057374022224\n","Epoch: 715 \\| Time: 0 \\| Train Loss: 3.0264736696288836 \\| Test Loss: 1.8635922721527165 \\| Test Accuracy: 0.5957296807332058\n","Epoch: 716 \\| Time: 0 \\| Train Loss: 3.0103854087795385 \\| Test Loss: 1.8541633504654718 \\| Test Accuracy: 0.6055997582838151\n","Epoch: 717 \\| Time: 0 \\| Train Loss: 2.9946556557544923 \\| Test Loss: 1.8252907147223345 \\| Test Accuracy: 0.6359149964749723\n","Epoch: 718 \\| Time: 0 \\| Train Loss: 3.0397302826631205 \\| Test Loss: 1.8639434054174138 \\| Test Accuracy: 0.5957632524255547\n","Epoch: 719 \\| Time: 0 \\| Train Loss: 3.0086742527844486 \\| Test Loss: 1.8542299567374037 \\| Test Accuracy: 0.6060026185920032\n","Epoch: 720 \\| Time: 0 \\| Train Loss: 3.0020340013312725 \\| Test Loss: 1.8212875168722586 \\| Test Accuracy: 0.6411521804814181\n","Epoch: 721 \\| Time: 0 \\| Train Loss: 2.9927697916875817 \\| Test Loss: 1.8240093995573183 \\| Test Accuracy: 0.6375935810924229\n","Epoch: 722 \\| Time: 0 \\| Train Loss: 3.001030557128967 \\| Test Loss: 1.8180207761060527 \\| Test Accuracy: 0.6446436364857152\n","Epoch: 723 \\| Time: 0 \\| Train Loss: 3.0355035468110647 \\| Test Loss: 1.8362474175481838 \\| Test Accuracy: 0.6246349078457045\n","Epoch: 724 \\| Time: 0 \\| Train Loss: 2.9992027418380336 \\| Test Loss: 1.838267566820071 \\| Test Accuracy: 0.6240306173834222\n","Epoch: 725 \\| Time: 0 \\| Train Loss: 3.0035864380941644 \\| Test Loss: 1.850805882732244 \\| Test Accuracy: 0.6082519219793869\n","Epoch: 726 \\| Time: 0 \\| Train Loss: 2.9888860889745272 \\| Test Loss: 1.8663957666429838 \\| Test Accuracy: 0.5911975022660892\n","Epoch: 727 \\| Time: 0 \\| Train Loss: 3.010889344781684 \\| Test Loss: 1.8405327909494162 \\| Test Accuracy: 0.6206734481485212\n","Epoch: 728 \\| Time: 0 \\| Train Loss: 3.008512937763724 \\| Test Loss: 1.8326222870994535 \\| Test Accuracy: 0.6297713767751032\n","Epoch: 729 \\| Time: 0 \\| Train Loss: 3.0092162327031455 \\| Test Loss: 1.868447679306816 \\| Test Accuracy: 0.5923725114983046\n","Epoch: 730 \\| Time: 0 \\| Train Loss: 3.0142573709861558 \\| Test Loss: 1.8361000774244383 \\| Test Accuracy: 0.6245341927686575\n","Epoch: 731 \\| Time: 0 \\| Train Loss: 3.017338873951855 \\| Test Loss: 1.8634368958902974 \\| Test Accuracy: 0.5980125558129386\n","Epoch: 732 \\| Time: 0 \\| Train Loss: 3.0007482165004093 \\| Test Loss: 1.8183748875564771 \\| Test Accuracy: 0.6447107798704133\n","Epoch: 733 \\| Time: 0 \\| Train Loss: 2.996116967184989 \\| Test Loss: 1.8444494039715615 \\| Test Accuracy: 0.6177527109141572\n","Epoch: 734 \\| Time: 0 \\| Train Loss: 3.002201202186934 \\| Test Loss: 1.8194464168835096 \\| Test Accuracy: 0.6417564709437003\n","Epoch: 735 \\| Time: 0 \\| Train Loss: 2.9890370328288824 \\| Test Loss: 1.8612310139406392 \\| Test Accuracy: 0.599355423506899\n","Epoch: 736 \\| Time: 0 \\| Train Loss: 3.0160765516031054 \\| Test Loss: 1.8749840193040381 \\| Test Accuracy: 0.5844160204115889\n","Epoch: 737 \\| Time: 0 \\| Train Loss: 3.0199482604433183 \\| Test Loss: 1.8335901775073595 \\| Test Accuracy: 0.6279249336959076\n","Epoch: 738 \\| Time: 0 \\| Train Loss: 2.99483171650208 \\| Test Loss: 1.8201261973688019 \\| Test Accuracy: 0.6416893275590022\n","Epoch: 739 \\| Time: 0 \\| Train Loss: 3.014587952416967 \\| Test Loss: 1.8598894464099867 \\| Test Accuracy: 0.5985832745828717\n","Epoch: 740 \\| Time: 0 \\| Train Loss: 3.0093386603723973 \\| Test Loss: 1.827795085477215 \\| Test Accuracy: 0.6345049853963138\n","Epoch: 741 \\| Time: 0 \\| Train Loss: 2.9819769147708257 \\| Test Loss: 1.8317991868620778 \\| Test Accuracy: 0.6286970826199348\n","Epoch: 742 \\| Time: 0 \\| Train Loss: 3.010336463058507 \\| Test Loss: 1.8324827114399922 \\| Test Accuracy: 0.6299056635444993\n","Epoch: 743 \\| Time: 0 \\| Train Loss: 3.035476538428255 \\| Test Loss: 1.8485477697184158 \\| Test Accuracy: 0.6111055158290529\n","Epoch: 744 \\| Time: 0 \\| Train Loss: 3.0157261842388023 \\| Test Loss: 1.883282589298461 \\| Test Accuracy: 0.5752509484003089\n","Epoch: 745 \\| Time: 0 \\| Train Loss: 3.013000683595166 \\| Test Loss: 1.8366755323860267 \\| Test Accuracy: 0.6252056266156377\n","Epoch: 746 \\| Time: 0 \\| Train Loss: 3.0057205924095287 \\| Test Loss: 1.8765367494632246 \\| Test Accuracy: 0.5803874173297076\n","Epoch: 747 \\| Time: 0 \\| Train Loss: 3.03087423979136 \\| Test Loss: 1.84026559125712 \\| Test Accuracy: 0.6214120253801995\n","Epoch: 748 \\| Time: 0 \\| Train Loss: 3.011659800029328 \\| Test Loss: 1.834055583364462 \\| Test Accuracy: 0.6278577903112096\n","Epoch: 749 \\| Time: 0 \\| Train Loss: 2.992985084639801 \\| Test Loss: 1.8223085572279574 \\| Test Accuracy: 0.6403464598650418\n","Epoch: 750 \\| Time: 0 \\| Train Loss: 2.983731677408574 \\| Test Loss: 1.8235533273271225 \\| Test Accuracy: 0.638063584785309\n","Epoch: 751 \\| Time: 0 \\| Train Loss: 2.990430171118111 \\| Test Loss: 1.8560679634241588 \\| Test Accuracy: 0.6046261792056937\n","Epoch: 752 \\| Time: 0 \\| Train Loss: 3.028499897553199 \\| Test Loss: 1.836434338737455 \\| Test Accuracy: 0.6256756303085238\n","Epoch: 753 \\| Time: 0 \\| Train Loss: 3.000157019285322 \\| Test Loss: 1.8616389225480894 \\| Test Accuracy: 0.6010004364320005\n","Epoch: 754 \\| Time: 0 \\| Train Loss: 2.9935843119730148 \\| Test Loss: 1.8312696418025463 \\| Test Accuracy: 0.6310806727767146\n","Epoch: 755 \\| Time: 0 \\| Train Loss: 2.9724157138497787 \\| Test Loss: 1.8266160882082108 \\| Test Accuracy: 0.637996441400611\n","Epoch: 756 \\| Time: 0 \\| Train Loss: 3.0348520126474345 \\| Test Loss: 1.8456305122170837 \\| Test Accuracy: 0.6139255379863698\n","Epoch: 757 \\| Time: 0 \\| Train Loss: 3.0026465757040377 \\| Test Loss: 1.8944173826168536 \\| Test Accuracy: 0.563870144693994\n","Epoch: 758 \\| Time: 0 \\| Train Loss: 3.00097225193737 \\| Test Loss: 1.8263617191191908 \\| Test Accuracy: 0.6356799946285292\n","Epoch: 759 \\| Time: 0 \\| Train Loss: 2.9900033783057682 \\| Test Loss: 1.8226393532855316 \\| Test Accuracy: 0.6375935810924229\n","Epoch: 760 \\| Time: 0 \\| Train Loss: 3.002230755106502 \\| Test Loss: 1.816660062437917 \\| Test Accuracy: 0.643837915869339\n","Epoch: 761 \\| Time: 0 \\| Train Loss: 2.9974092479711505 \\| Test Loss: 1.8886001212402475 \\| Test Accuracy: 0.5722966394735959\n","Epoch: 762 \\| Time: 0 \\| Train Loss: 2.987227250167151 \\| Test Loss: 1.8378177681705983 \\| Test Accuracy: 0.6243999059992614\n","Epoch: 763 \\| Time: 0 \\| Train Loss: 2.995819931665803 \\| Test Loss: 1.8394707088306739 \\| Test Accuracy: 0.6237284721522812\n","Epoch: 764 \\| Time: 0 \\| Train Loss: 2.9844250579969636 \\| Test Loss: 1.8229798810164817 \\| Test Accuracy: 0.6372578641689327\n","Epoch: 765 \\| Time: 0 \\| Train Loss: 3.0007861143480365 \\| Test Loss: 1.86574125289917 \\| Test Accuracy: 0.590391781649713\n","Epoch: 766 \\| Time: 0 \\| Train Loss: 3.024072202371936 \\| Test Loss: 1.8304006848724104 \\| Test Accuracy: 0.6304428106220834\n","Epoch: 767 \\| Time: 0 \\| Train Loss: 2.9940227628718827 \\| Test Loss: 1.8283520931849664 \\| Test Accuracy: 0.6331956893947024\n","Epoch: 768 \\| Time: 0 \\| Train Loss: 2.994088058675986 \\| Test Loss: 1.8529647645008922 \\| Test Accuracy: 0.6067076241313325\n","Epoch: 769 \\| Time: 0 \\| Train Loss: 3.019806847613805 \\| Test Loss: 1.8269547367300598 \\| Test Accuracy: 0.6342028401651727\n","Epoch: 770 \\| Time: 0 \\| Train Loss: 2.962690815881997 \\| Test Loss: 1.8310022047149266 \\| Test Accuracy: 0.6275220733877195\n","Epoch: 771 \\| Time: 0 \\| Train Loss: 3.0047236456770423 \\| Test Loss: 1.8288324283427946 \\| Test Accuracy: 0.6310471010843657\n","Epoch: 772 \\| Time: 0 \\| Train Loss: 3.007796561232512 \\| Test Loss: 1.8456593838884083 \\| Test Accuracy: 0.6146976869103972\n","Epoch: 773 \\| Time: 0 \\| Train Loss: 2.995113993150544 \\| Test Loss: 1.8303804366885337 \\| Test Accuracy: 0.632121395239534\n","Epoch: 774 \\| Time: 0 \\| Train Loss: 3.0033667213547854 \\| Test Loss: 1.8350684821861496 \\| Test Accuracy: 0.6251049115385907\n","Epoch: 775 \\| Time: 0 \\| Train Loss: 2.9896980508752407 \\| Test Loss: 1.8484047502918817 \\| Test Accuracy: 0.6136905361399269\n","Epoch: 776 \\| Time: 0 \\| Train Loss: 2.976070376054895 \\| Test Loss: 1.8364849018948273 \\| Test Accuracy: 0.6248363379997985\n","Epoch: 777 \\| Time: 0 \\| Train Loss: 3.011760245451515 \\| Test Loss: 1.8321223985483719 \\| Test Accuracy: 0.6292342296975191\n","Epoch: 778 \\| Time: 0 \\| Train Loss: 2.981778034889879 \\| Test Loss: 1.8212088709737098 \\| Test Accuracy: 0.6403800315573908\n","Epoch: 779 \\| Time: 0 \\| Train Loss: 2.977636533292756 \\| Test Loss: 1.8259898925543854 \\| Test Accuracy: 0.6355457078591332\n","Epoch: 780 \\| Time: 0 \\| Train Loss: 2.9715008743553937 \\| Test Loss: 1.8481278562750427 \\| Test Accuracy: 0.6132876758317387\n","Epoch: 781 \\| Time: 0 \\| Train Loss: 2.9869132847875433 \\| Test Loss: 1.8367722494919412 \\| Test Accuracy: 0.6256084869238259\n","Epoch: 782 \\| Time: 0 \\| Train Loss: 3.009180559553336 \\| Test Loss: 1.8418935670361498 \\| Test Accuracy: 0.6194984389163057\n","Epoch: 783 \\| Time: 0 \\| Train Loss: 3.0332199880999138 \\| Test Loss: 1.8727649869836962 \\| Test Accuracy: 0.5854567428744083\n","Epoch: 784 \\| Time: 0 \\| Train Loss: 2.968196945914042 \\| Test Loss: 1.8369076093379009 \\| Test Accuracy: 0.6251720549232886\n","Epoch: 785 \\| Time: 0 \\| Train Loss: 3.0079163967555824 \\| Test Loss: 1.839435941159981 \\| Test Accuracy: 0.6216805989189915\n","Epoch: 786 \\| Time: 0 \\| Train Loss: 3.0241209159148235 \\| Test Loss: 1.8308779431003357 \\| Test Accuracy: 0.6304428106220834\n","Epoch: 787 \\| Time: 0 \\| Train Loss: 2.9677744431244797 \\| Test Loss: 1.8276628194448774 \\| Test Accuracy: 0.6342028401651727\n","Epoch: 788 \\| Time: 0 \\| Train Loss: 3.0009992493524384 \\| Test Loss: 1.8561343138821647 \\| Test Accuracy: 0.6038876019740155\n","Epoch: 789 \\| Time: 0 \\| Train Loss: 3.02363075996875 \\| Test Loss: 1.8417903594193028 \\| Test Accuracy: 0.6177862826065061\n","Epoch: 790 \\| Time: 0 \\| Train Loss: 3.069043970999533 \\| Test Loss: 1.8485395580913888 \\| Test Accuracy: 0.6102997952126766\n","Epoch: 791 \\| Time: 0 \\| Train Loss: 3.0076378360042 \\| Test Loss: 1.841549145817245 \\| Test Accuracy: 0.6179541410682513\n","Epoch: 792 \\| Time: 0 \\| Train Loss: 3.0102908297738584 \\| Test Loss: 1.8450946516233453 \\| Test Accuracy: 0.6158726961426125\n","Epoch: 793 \\| Time: 0 \\| Train Loss: 2.995027136277317 \\| Test Loss: 1.8322670101607819 \\| Test Accuracy: 0.628898512774029\n","Epoch: 794 \\| Time: 0 \\| Train Loss: 2.9987311960836873 \\| Test Loss: 1.8432292293581327 \\| Test Accuracy: 0.6168127035283849\n","Epoch: 795 \\| Time: 0 \\| Train Loss: 2.982219842323803 \\| Test Loss: 1.8459545916242148 \\| Test Accuracy: 0.6160069829120086\n","Epoch: 796 \\| Time: 0 \\| Train Loss: 3.029065270783715 \\| Test Loss: 1.846134638070037 \\| Test Accuracy: 0.6149998321415383\n","Epoch: 797 \\| Time: 0 \\| Train Loss: 2.9917765380983576 \\| Test Loss: 1.8351500776192662 \\| Test Accuracy: 0.6248363379997985\n","Epoch: 798 \\| Time: 0 \\| Train Loss: 3.00295165190592 \\| Test Loss: 1.8505163422981556 \\| Test Accuracy: 0.6105683687514688\n","Epoch: 799 \\| Time: 0 \\| Train Loss: 2.988271553926081 \\| Test Loss: 1.850821170684094 \\| Test Accuracy: 0.6071776278242186\n","Epoch: 800 \\| Time: 0 \\| Train Loss: 3.0086097966211955 \\| Test Loss: 1.8242581586469397 \\| Test Accuracy: 0.6383321583241012\n","Epoch: 801 \\| Time: 0 \\| Train Loss: 3.0108554579593987 \\| Test Loss: 1.8364343781327996 \\| Test Accuracy: 0.6237284721522812\n","Epoch: 802 \\| Time: 0 \\| Train Loss: 2.9961536527278736 \\| Test Loss: 1.8450446778612588 \\| Test Accuracy: 0.6161076979890556\n","Epoch: 803 \\| Time: 0 \\| Train Loss: 3.0168517405809494 \\| Test Loss: 1.8487534149521923 \\| Test Accuracy: 0.6122805250612683\n","Epoch: 804 \\| Time: 0 \\| Train Loss: 3.002343564259351 \\| Test Loss: 1.835682029887842 \\| Test Accuracy: 0.6254742001544298\n","Epoch: 805 \\| Time: 0 \\| Train Loss: 2.9898458091789397 \\| Test Loss: 1.837340104733414 \\| Test Accuracy: 0.623057038305301\n","Epoch: 806 \\| Time: 0 \\| Train Loss: 3.010502316879205 \\| Test Loss: 1.862142898494082 \\| Test Accuracy: 0.5976096955047504\n","Epoch: 807 \\| Time: 0 \\| Train Loss: 3.0346742256949137 \\| Test Loss: 1.8301893288485482 \\| Test Accuracy: 0.6307785275455736\n","Epoch: 808 \\| Time: 0 \\| Train Loss: 2.9690415885784778 \\| Test Loss: 1.8216996126420508 \\| Test Accuracy: 0.6419914727901433\n","Epoch: 809 \\| Time: 0 \\| Train Loss: 3.0026460134000197 \\| Test Loss: 1.839355876517398 \\| Test Accuracy: 0.6227884647665088\n","Epoch: 810 \\| Time: 0 \\| Train Loss: 2.9936010839131004 \\| Test Loss: 1.8509293743469173 \\| Test Accuracy: 0.6110048007520059\n","Epoch: 811 \\| Time: 0 \\| Train Loss: 3.0225227245373807 \\| Test Loss: 1.8297046888539719 \\| Test Accuracy: 0.631987108470138\n","Epoch: 812 \\| Time: 0 \\| Train Loss: 2.9875331161699394 \\| Test Loss: 1.825064629444237 \\| Test Accuracy: 0.6361835700137644\n","Epoch: 813 \\| Time: 0 \\| Train Loss: 2.9969970302655846 \\| Test Loss: 1.8454867596278375 \\| Test Accuracy: 0.6153355490650284\n","Epoch: 814 \\| Time: 0 \\| Train Loss: 3.0036685655074478 \\| Test Loss: 1.8485889695744657 \\| Test Accuracy: 0.6118105213683822\n","Epoch: 815 \\| Time: 0 \\| Train Loss: 3.0383222801039644 \\| Test Loss: 1.8632744893495616 \\| Test Accuracy: 0.5963339711954879\n","Epoch: 816 \\| Time: 0 \\| Train Loss: 3.0245891285281274 \\| Test Loss: 1.8537480892541582 \\| Test Accuracy: 0.6058683318226071\n","Epoch: 817 \\| Time: 0 \\| Train Loss: 2.9815922411796882 \\| Test Loss: 1.825547228555311 \\| Test Accuracy: 0.6354114210897371\n","Epoch: 818 \\| Time: 0 \\| Train Loss: 3.0016920000847542 \\| Test Loss: 1.8805342411278656 \\| Test Accuracy: 0.5784066874811159\n","Epoch: 819 \\| Time: 0 \\| Train Loss: 3.003845685493666 \\| Test Loss: 1.8514847013571742 \\| Test Accuracy: 0.6077819182865009\n","Epoch: 820 \\| Time: 0 \\| Train Loss: 3.0120343972324664 \\| Test Loss: 1.8161559442593815 \\| Test Accuracy: 0.6460536475643737\n","Epoch: 821 \\| Time: 0 \\| Train Loss: 3.0207688184033312 \\| Test Loss: 1.900885875132974 \\| Test Accuracy: 0.5565515157619095\n","Epoch: 822 \\| Time: 0 \\| Train Loss: 2.971200676700265 \\| Test Loss: 1.8200430481218985 \\| Test Accuracy: 0.6417228992513513\n","Epoch: 823 \\| Time: 0 \\| Train Loss: 2.995097031120563 \\| Test Loss: 1.8329251886949007 \\| Test Accuracy: 0.628965656158727\n","Epoch: 824 \\| Time: 0 \\| Train Loss: 2.983748026009244 \\| Test Loss: 1.8339936518362152 \\| Test Accuracy: 0.6276899318494645\n","Epoch: 825 \\| Time: 0 \\| Train Loss: 3.022565316742517 \\| Test Loss: 1.857888466298836 \\| Test Accuracy: 0.602108302279518\n","Epoch: 826 \\| Time: 0 \\| Train Loss: 2.999748287019391 \\| Test Loss: 1.82549989274643 \\| Test Accuracy: 0.6360492832443684\n","Epoch: 827 \\| Time: 0 \\| Train Loss: 2.981141740292859 \\| Test Loss: 1.8329538444592717 \\| Test Accuracy: 0.6284285090811428\n","Epoch: 828 \\| Time: 0 \\| Train Loss: 2.9602357624994906 \\| Test Loss: 1.8471335551257808 \\| Test Accuracy: 0.6151341189109343\n","Epoch: 829 \\| Time: 0 \\| Train Loss: 3.0148769155614663 \\| Test Loss: 1.8475174218288306 \\| Test Accuracy: 0.6116762345989861\n","Epoch: 830 \\| Time: 0 \\| Train Loss: 2.989412540031287 \\| Test Loss: 1.8097096476943708 \\| Test Accuracy: 0.652902272803572\n","Epoch: 831 \\| Time: 0 \\| Train Loss: 3.0072508030797533 \\| Test Loss: 1.8476903141823962 \\| Test Accuracy: 0.6146641152180481\n","Epoch: 832 \\| Time: 0 \\| Train Loss: 2.981730518681182 \\| Test Loss: 1.8331611279254307 \\| Test Accuracy: 0.6277906469265115\n","Epoch: 833 \\| Time: 0 \\| Train Loss: 3.010284604990229 \\| Test Loss: 1.8466421294110016 \\| Test Accuracy: 0.6126833853694564\n","Epoch: 834 \\| Time: 0 \\| Train Loss: 2.9860936956521678 \\| Test Loss: 1.8615259387462435 \\| Test Accuracy: 0.5972739785812603\n","Epoch: 835 \\| Time: 0 \\| Train Loss: 2.9634393861320016 \\| Test Loss: 1.8174037442186872 \\| Test Accuracy: 0.6437372007922919\n","Epoch: 836 \\| Time: 0 \\| Train Loss: 3.008015973637033 \\| Test Loss: 1.8475696436836995 \\| Test Accuracy: 0.6134219626011347\n","Epoch: 837 \\| Time: 0 \\| Train Loss: 2.986124985239179 \\| Test Loss: 1.8364633905017835 \\| Test Accuracy: 0.6240641890757713\n","Epoch: 838 \\| Time: 0 \\| Train Loss: 2.998453771707103 \\| Test Loss: 1.8627529998705623 \\| Test Accuracy: 0.5952261053479706\n","Epoch: 839 \\| Time: 0 \\| Train Loss: 2.9976079735034746 \\| Test Loss: 1.8304295232879246 \\| Test Accuracy: 0.6301742370832913\n","Epoch: 840 \\| Time: 0 \\| Train Loss: 3.0069770818157586 \\| Test Loss: 1.8314589497357479 \\| Test Accuracy: 0.6286970826199348\n","Epoch: 841 \\| Time: 0 \\| Train Loss: 2.9985729772785104 \\| Test Loss: 1.8243181792451588 \\| Test Accuracy: 0.6365864303219525\n","Epoch: 842 \\| Time: 0 \\| Train Loss: 2.9867252808379083 \\| Test Loss: 1.8378815297916724 \\| Test Accuracy: 0.6225870346124148\n","Epoch: 843 \\| Time: 0 \\| Train Loss: 3.004538866224978 \\| Test Loss: 1.8601327295467065 \\| Test Accuracy: 0.5996575687380401\n","Epoch: 844 \\| Time: 0 \\| Train Loss: 2.988970331591455 \\| Test Loss: 1.8616318984093072 \\| Test Accuracy: 0.5978111256588444\n","Epoch: 845 \\| Time: 0 \\| Train Loss: 2.974676323434103 \\| Test Loss: 1.8603096719463497 \\| Test Accuracy: 0.6006311478161614\n","Epoch: 846 \\| Time: 0 \\| Train Loss: 2.993342633315063 \\| Test Loss: 1.8677239842680902 \\| Test Accuracy: 0.5911303588813912\n","Epoch: 847 \\| Time: 0 \\| Train Loss: 3.020620186898283 \\| Test Loss: 1.8347089101316592 \\| Test Accuracy: 0.6266156376942962\n","Epoch: 848 \\| Time: 0 \\| Train Loss: 3.0075579150094396 \\| Test Loss: 1.8522651251805187 \\| Test Accuracy: 0.6071776278242186\n","Epoch: 849 \\| Time: 0 \\| Train Loss: 2.9872103794841203 \\| Test Loss: 1.8148347287730597 \\| Test Accuracy: 0.6485379527982006\n","Epoch: 850 \\| Time: 0 \\| Train Loss: 3.0240140360165304 \\| Test Loss: 1.8236153780646591 \\| Test Accuracy: 0.6373921509383288\n","Epoch: 851 \\| Time: 0 \\| Train Loss: 2.9830029434854244 \\| Test Loss: 1.8263441037722412 \\| Test Accuracy: 0.6337328364722866\n","Epoch: 852 \\| Time: 0 \\| Train Loss: 2.991211841851338 \\| Test Loss: 1.8693119937258218 \\| Test Accuracy: 0.5896867761103837\n","Epoch: 853 \\| Time: 0 \\| Train Loss: 2.996787666742278 \\| Test Loss: 1.8372577500445648 \\| Test Accuracy: 0.6224191761506698\n","Epoch: 854 \\| Time: 0 \\| Train Loss: 3.0091833194633737 \\| Test Loss: 1.8219409556859552 \\| Test Accuracy: 0.6389700204787323\n","Epoch: 855 \\| Time: 0 \\| Train Loss: 2.997914226327287 \\| Test Loss: 1.8357869657835735 \\| Test Accuracy: 0.6256756303085238\n","Epoch: 856 \\| Time: 0 \\| Train Loss: 3.020516166949402 \\| Test Loss: 1.8308499386382204 \\| Test Accuracy: 0.6311478161614127\n","Epoch: 857 \\| Time: 0 \\| Train Loss: 3.0143024203924664 \\| Test Loss: 1.857999085868377 \\| Test Accuracy: 0.602343304125961\n","Epoch: 858 \\| Time: 0 \\| Train Loss: 2.975044966975596 \\| Test Loss: 1.8615132498638824 \\| Test Accuracy: 0.5988854198140128\n","Epoch: 859 \\| Time: 0 \\| Train Loss: 3.0048486757494355 \\| Test Loss: 1.8506914172561384 \\| Test Accuracy: 0.6102662235203277\n","Epoch: 860 \\| Time: 0 \\| Train Loss: 3.0026098085361705 \\| Test Loss: 1.831909418617707 \\| Test Accuracy: 0.6295363749286601\n","Epoch: 861 \\| Time: 0 \\| Train Loss: 3.0045469098555007 \\| Test Loss: 1.8314524873643474 \\| Test Accuracy: 0.6308792426226206\n","Epoch: 862 \\| Time: 0 \\| Train Loss: 3.003291178440908 \\| Test Loss: 1.836986164166692 \\| Test Accuracy: 0.6243999059992614\n","Epoch: 863 \\| Time: 0 \\| Train Loss: 2.9993420771266455 \\| Test Loss: 1.825249450912803 \\| Test Accuracy: 0.6356464229361802\n","Epoch: 864 \\| Time: 0 \\| Train Loss: 3.0046373146780803 \\| Test Loss: 1.828789403510196 \\| Test Accuracy: 0.6334978346258435\n","Epoch: 865 \\| Time: 0 \\| Train Loss: 3.004855762323786 \\| Test Loss: 1.8194669781836317 \\| Test Accuracy: 0.6430321952529627\n","Epoch: 866 \\| Time: 0 \\| Train Loss: 2.979899885452031 \\| Test Loss: 1.848835795222434 \\| Test Accuracy: 0.6105012253667708\n","Epoch: 867 \\| Time: 0 \\| Train Loss: 3.0152462815592824 \\| Test Loss: 1.8491439159336007 \\| Test Accuracy: 0.6104005102897238\n","Epoch: 868 \\| Time: 0 \\| Train Loss: 2.9981903711433633 \\| Test Loss: 1.8627548468471085 \\| Test Accuracy: 0.5974754087353543\n","Epoch: 869 \\| Time: 0 \\| Train Loss: 2.9781313298894028 \\| Test Loss: 1.839205808905573 \\| Test Accuracy: 0.6211434518414073\n","Epoch: 870 \\| Time: 0 \\| Train Loss: 2.9956118408111814 \\| Test Loss: 1.8407352671602766 \\| Test Accuracy: 0.6197670124550979\n","Epoch: 871 \\| Time: 0 \\| Train Loss: 3.028923071733404 \\| Test Loss: 1.857434782347454 \\| Test Accuracy: 0.6034511699734784\n","Epoch: 872 \\| Time: 0 \\| Train Loss: 2.993117393002676 \\| Test Loss: 1.8581964585914121 \\| Test Accuracy: 0.6004968610467654\n","Epoch: 873 \\| Time: 0 \\| Train Loss: 3.0183038724491436 \\| Test Loss: 1.8301387979237307 \\| Test Accuracy: 0.6318192500083929\n","Epoch: 874 \\| Time: 0 \\| Train Loss: 2.988462208518456 \\| Test Loss: 1.8270835334139321 \\| Test Accuracy: 0.6339006949340317\n","Epoch: 875 \\| Time: 0 \\| Train Loss: 3.0410392075767088 \\| Test Loss: 1.8483451922052407 \\| Test Accuracy: 0.6139591096787189\n","Epoch: 876 \\| Time: 0 \\| Train Loss: 2.9851601062146473 \\| Test Loss: 1.8239516746332718 \\| Test Accuracy: 0.6391043072481284\n","Epoch: 877 \\| Time: 0 \\| Train Loss: 3.0378844474994655 \\| Test Loss: 1.864874271875799 \\| Test Accuracy: 0.5957968241179038\n","Epoch: 878 \\| Time: 0 \\| Train Loss: 3.00176233144393 \\| Test Loss: 1.8377524378985295 \\| Test Accuracy: 0.6232248967670461\n","Epoch: 879 \\| Time: 0 \\| Train Loss: 3.001746316758218 \\| Test Loss: 1.8461722918334438 \\| Test Accuracy: 0.614294826602209\n","Epoch: 880 \\| Time: 0 \\| Train Loss: 3.023268182679552 \\| Test Loss: 1.8195372113854076 \\| Test Accuracy: 0.641185752173767\n","Epoch: 881 \\| Time: 0 \\| Train Loss: 3.024633375138723 \\| Test Loss: 1.8440588456878335 \\| Test Accuracy: 0.6167455601436869\n","Epoch: 882 \\| Time: 0 \\| Train Loss: 2.9770133154554577 \\| Test Loss: 1.8315953502327587 \\| Test Accuracy: 0.6284620807734917\n","Epoch: 883 \\| Time: 0 \\| Train Loss: 3.0117152499395052 \\| Test Loss: 1.8232241341996092 \\| Test Accuracy: 0.6374592943230268\n","Epoch: 884 \\| Time: 0 \\| Train Loss: 3.013030795985736 \\| Test Loss: 1.8640430142439486 \\| Test Accuracy: 0.5963339711954879\n","Epoch: 885 \\| Time: 0 \\| Train Loss: 2.9916589652844845 \\| Test Loss: 1.853807682131493 \\| Test Accuracy: 0.6072783429012657\n","Epoch: 886 \\| Time: 0 \\| Train Loss: 2.9706758320691637 \\| Test Loss: 1.8430130087766525 \\| Test Accuracy: 0.617081277067177\n","Epoch: 887 \\| Time: 0 \\| Train Loss: 3.002537259586858 \\| Test Loss: 1.8432983380018897 \\| Test Accuracy: 0.6178534259912042\n","Epoch: 888 \\| Time: 0 \\| Train Loss: 2.9923765545820524 \\| Test Loss: 1.8294354810223559 \\| Test Accuracy: 0.6314835330849028\n","Epoch: 889 \\| Time: 0 \\| Train Loss: 3.0119738550841415 \\| Test Loss: 1.834304610547078 \\| Test Accuracy: 0.6271863564642294\n","Epoch: 890 \\| Time: 0 \\| Train Loss: 2.995776486659424 \\| Test Loss: 1.8199514907828729 \\| Test Accuracy: 0.6423271897136335\n","Epoch: 891 \\| Time: 0 \\| Train Loss: 2.978264697835433 \\| Test Loss: 1.8234551167795074 \\| Test Accuracy: 0.6376942961694699\n","Epoch: 892 \\| Time: 0 \\| Train Loss: 3.027140139503623 \\| Test Loss: 1.861533128140822 \\| Test Accuracy: 0.5981468425823345\n","Epoch: 893 \\| Time: 0 \\| Train Loss: 3.0021323275702967 \\| Test Loss: 1.8569448592836766 \\| Test Accuracy: 0.60237687581831\n","Epoch: 894 \\| Time: 0 \\| Train Loss: 2.975041525568811 \\| Test Loss: 1.8186044442295517 \\| Test Accuracy: 0.6423943330983315\n","Epoch: 895 \\| Time: 0 \\| Train Loss: 2.995278665458088 \\| Test Loss: 1.8476100694468094 \\| Test Accuracy: 0.6122469533689193\n","Epoch: 896 \\| Time: 0 \\| Train Loss: 3.0214203160396003 \\| Test Loss: 1.8287710539772786 \\| Test Accuracy: 0.6333635478564474\n","Epoch: 897 \\| Time: 0 \\| Train Loss: 3.0009854971263468 \\| Test Loss: 1.8253160259754362 \\| Test Accuracy: 0.6354785644744352\n","Epoch: 898 \\| Time: 0 \\| Train Loss: 2.9922901655337553 \\| Test Loss: 1.8333443101383586 \\| Test Accuracy: 0.6271527847718803\n","Epoch: 899 \\| Time: 0 \\| Train Loss: 2.9980873326535766 \\| Test Loss: 1.841385125090636 \\| Test Accuracy: 0.619934870916843\n","Epoch: 900 \\| Time: 0 \\| Train Loss: 2.997907416982849 \\| Test Loss: 1.8310442758732088 \\| Test Accuracy: 0.6307113841608756\n","Epoch: 901 \\| Time: 0 \\| Train Loss: 3.0161913061631362 \\| Test Loss: 1.8212402281331401 \\| Test Accuracy: 0.6406150334038339\n","Epoch: 902 \\| Time: 0 \\| Train Loss: 3.0042609101740174 \\| Test Loss: 1.8262326031795386 \\| Test Accuracy: 0.6348407023198039\n","Epoch: 903 \\| Time: 0 \\| Train Loss: 2.981948496934742 \\| Test Loss: 1.8407781318533574 \\| Test Accuracy: 0.620404874609729\n","Epoch: 904 \\| Time: 0 \\| Train Loss: 3.0001381570956545 \\| Test Loss: 1.8405683802944397 \\| Test Accuracy: 0.620102729378588\n","Epoch: 905 \\| Time: 0 \\| Train Loss: 2.9822083313743626 \\| Test Loss: 1.8443555187258085 \\| Test Accuracy: 0.61698056199013\n","Epoch: 906 \\| Time: 0 \\| Train Loss: 3.028609545584961 \\| Test Loss: 1.8437069690278671 \\| Test Accuracy: 0.617282707221271\n","Epoch: 907 \\| Time: 0 \\| Train Loss: 3.0000531654130507 \\| Test Loss: 1.8586070629660152 \\| Test Accuracy: 0.602108302279518\n","Epoch: 908 \\| Time: 0 \\| Train Loss: 3.0327908530412784 \\| Test Loss: 1.851397302017703 \\| Test Accuracy: 0.608319065364085\n","Epoch: 909 \\| Time: 0 \\| Train Loss: 2.9887995109355296 \\| Test Loss: 1.8222837862538677 \\| Test Accuracy: 0.6393057374022224\n","Epoch: 910 \\| Time: 0 \\| Train Loss: 2.985982873666199 \\| Test Loss: 1.8299414717588303 \\| Test Accuracy: 0.6307449558532245\n","Epoch: 911 \\| Time: 0 \\| Train Loss: 3.0127663808205316 \\| Test Loss: 1.871645145661841 \\| Test Accuracy: 0.5869338973377648\n","Epoch: 912 \\| Time: 0 \\| Train Loss: 2.9813068520208597 \\| Test Loss: 1.825037865679663 \\| Test Accuracy: 0.6368885755530936\n","Epoch: 913 \\| Time: 0 \\| Train Loss: 3.0033083655259967 \\| Test Loss: 1.8458392753109911 \\| Test Accuracy: 0.6133548192164368\n","Epoch: 914 \\| Time: 0 \\| Train Loss: 2.9735718420918404 \\| Test Loss: 1.82597946762527 \\| Test Accuracy: 0.6361835700137644\n","Epoch: 915 \\| Time: 0 \\| Train Loss: 2.9909016629833656 \\| Test Loss: 1.831494726336565 \\| Test Accuracy: 0.629066371235774\n","Epoch: 916 \\| Time: 0 \\| Train Loss: 2.9962028763529056 \\| Test Loss: 1.8543304194708239 \\| Test Accuracy: 0.605062611206231\n","Epoch: 917 \\| Time: 0 \\| Train Loss: 3.01008566284417 \\| Test Loss: 1.857212546045688 \\| Test Accuracy: 0.6031490247423372\n","Epoch: 918 \\| Time: 0 \\| Train Loss: 3.0000554844222775 \\| Test Loss: 1.8317507142161096 \\| Test Accuracy: 0.6285963675428878\n","Epoch: 919 \\| Time: 0 \\| Train Loss: 2.9773706559463378 \\| Test Loss: 1.8063429542877132 \\| Test Accuracy: 0.6565615872696142\n","Epoch: 920 \\| Time: 0 \\| Train Loss: 3.0232199457070656 \\| Test Loss: 1.850797615849409 \\| Test Accuracy: 0.6105683687514688\n","Epoch: 921 \\| Time: 0 \\| Train Loss: 3.0151095874646687 \\| Test Loss: 1.8326356247259312 \\| Test Accuracy: 0.6287642260046329\n","Epoch: 922 \\| Time: 0 \\| Train Loss: 2.9888455774792884 \\| Test Loss: 1.818284397984779 \\| Test Accuracy: 0.6434350555611509\n","Epoch: 923 \\| Time: 0 \\| Train Loss: 3.026362368575194 \\| Test Loss: 1.835653162821168 \\| Test Accuracy: 0.6268506395407393\n","Epoch: 924 \\| Time: 0 \\| Train Loss: 3.010431182233406 \\| Test Loss: 1.8405574882490952 \\| Test Accuracy: 0.6203377312250311\n","Epoch: 925 \\| Time: 0 \\| Train Loss: 3.0240848690871154 \\| Test Loss: 1.8631039177399336 \\| Test Accuracy: 0.5976432671970994\n","Epoch: 926 \\| Time: 0 \\| Train Loss: 3.018344801830135 \\| Test Loss: 1.8333500219517 \\| Test Accuracy: 0.6279920770806057\n","Epoch: 927 \\| Time: 0 \\| Train Loss: 3.009434400295042 \\| Test Loss: 1.8370691123438496 \\| Test Accuracy: 0.6237956155369793\n","Epoch: 928 \\| Time: 0 \\| Train Loss: 3.0027587785090923 \\| Test Loss: 1.8428512305149194 \\| Test Accuracy: 0.6182898579917414\n","Epoch: 929 \\| Time: 0 \\| Train Loss: 2.9992280266193188 \\| Test Loss: 1.8254757029815805 \\| Test Accuracy: 0.6346392721657099\n","Epoch: 930 \\| Time: 0 \\| Train Loss: 2.965132509689123 \\| Test Loss: 1.839635424347906 \\| Test Accuracy: 0.620404874609729\n","Epoch: 931 \\| Time: 0 \\| Train Loss: 3.0236009918836313 \\| Test Loss: 1.8647000912433018 \\| Test Accuracy: 0.5949239601168295\n","Epoch: 932 \\| Time: 0 \\| Train Loss: 3.0003705661172697 \\| Test Loss: 1.8386069510627714 \\| Test Accuracy: 0.6222848893812737\n","Epoch: 933 \\| Time: 0 \\| Train Loss: 2.979045770616925 \\| Test Loss: 1.8191764329124418 \\| Test Accuracy: 0.6427636217141706\n","Epoch: 934 \\| Time: 0 \\| Train Loss: 2.9994597876375884 \\| Test Loss: 1.8583947957329483 \\| Test Accuracy: 0.6017054419713298\n","Epoch: 935 \\| Time: 0 \\| Train Loss: 2.956035989735272 \\| Test Loss: 1.8226259337985975 \\| Test Accuracy: 0.6404807466344379\n","Epoch: 936 \\| Time: 0 \\| Train Loss: 3.010139414770964 \\| Test Loss: 1.836277231638012 \\| Test Accuracy: 0.6217477423036896\n","Epoch: 937 \\| Time: 0 \\| Train Loss: 3.000913102334594 \\| Test Loss: 1.818287965565792 \\| Test Accuracy: 0.6433343404841038\n","Epoch: 938 \\| Time: 0 \\| Train Loss: 3.0060181414077145 \\| Test Loss: 1.838610927946066 \\| Test Accuracy: 0.6238963306140263\n","Epoch: 939 \\| Time: 0 \\| Train Loss: 3.005124086535061 \\| Test Loss: 1.8184921424276328 \\| Test Accuracy: 0.6440057743310841\n","Epoch: 940 \\| Time: 0 \\| Train Loss: 3.0306888711557245 \\| Test Loss: 1.834791518076295 \\| Test Accuracy: 0.6265484943095981\n","Epoch: 941 \\| Time: 0 \\| Train Loss: 3.0053615541482337 \\| Test Loss: 1.8103842305523132 \\| Test Accuracy: 0.6518615503407527\n","Epoch: 942 \\| Time: 0 \\| Train Loss: 3.0063837776290625 \\| Test Loss: 1.8912965755094275 \\| Test Accuracy: 0.5673951723906402\n","Epoch: 943 \\| Time: 0 \\| Train Loss: 2.999342472451463 \\| Test Loss: 1.890510629686675 \\| Test Accuracy: 0.5686037533152046\n","Epoch: 944 \\| Time: 0 \\| Train Loss: 2.9916993541542194 \\| Test Loss: 1.8809342752710432 \\| Test Accuracy: 0.5768288179407124\n","Epoch: 945 \\| Time: 0 \\| Train Loss: 2.991213955357146 \\| Test Loss: 1.857746062360608 \\| Test Accuracy: 0.6000604290462282\n","Epoch: 946 \\| Time: 0 \\| Train Loss: 3.026023296992576 \\| Test Loss: 1.8295234129664213 \\| Test Accuracy: 0.6331956893947024\n","Epoch: 947 \\| Time: 0 \\| Train Loss: 3.007694919053291 \\| Test Loss: 1.8436778712170319 \\| Test Accuracy: 0.6165105582972438\n","Epoch: 948 \\| Time: 0 \\| Train Loss: 3.001855065563361 \\| Test Loss: 1.82883663852839 \\| Test Accuracy: 0.6324235404706752\n","Epoch: 949 \\| Time: 0 \\| Train Loss: 2.994680045394646 \\| Test Loss: 1.8507129230212755 \\| Test Accuracy: 0.611273374290798\n","Epoch: 950 \\| Time: 0 \\| Train Loss: 2.9991525208130265 \\| Test Loss: 1.8566896454970725 \\| Test Accuracy: 0.6026454493571021\n","Epoch: 951 \\| Time: 0 \\| Train Loss: 3.026586675539534 \\| Test Loss: 1.8515956652522598 \\| Test Accuracy: 0.6073454862859636\n","Epoch: 952 \\| Time: 0 \\| Train Loss: 2.994950587768184 \\| Test Loss: 1.8444611929005308 \\| Test Accuracy: 0.6167791318360358\n","Epoch: 953 \\| Time: 0 \\| Train Loss: 3.01470021033183 \\| Test Loss: 1.8517850555575457 \\| Test Accuracy: 0.6088562124416692\n","Epoch: 954 \\| Time: 0 \\| Train Loss: 2.993647148362651 \\| Test Loss: 1.8670558878280574 \\| Test Accuracy: 0.5918689361130695\n","Epoch: 955 \\| Time: 0 \\| Train Loss: 3.0251076879907965 \\| Test Loss: 1.846083516726678 \\| Test Accuracy: 0.6133883909087857\n","Epoch: 956 \\| Time: 0 \\| Train Loss: 3.00978530636859 \\| Test Loss: 1.8202207804749453 \\| Test Accuracy: 0.6422936180212845\n","Epoch: 957 \\| Time: 0 \\| Train Loss: 2.9873188219531577 \\| Test Loss: 1.8847249526322656 \\| Test Accuracy: 0.5762245274784302\n","Epoch: 958 \\| Time: 0 \\| Train Loss: 3.0041106755049594 \\| Test Loss: 1.8267786901907859 \\| Test Accuracy: 0.6335649780105415\n","Epoch: 959 \\| Time: 0 \\| Train Loss: 2.998048492680862 \\| Test Loss: 1.8394579795297124 \\| Test Accuracy: 0.6211770235337564\n","Epoch: 960 \\| Time: 0 \\| Train Loss: 3.0060198912348755 \\| Test Loss: 1.9058904514803907 \\| Test Accuracy: 0.5525900560647262\n","Epoch: 961 \\| Time: 0 \\| Train Loss: 3.000346635323933 \\| Test Loss: 1.8504763657443002 \\| Test Accuracy: 0.611273374290798\n","Epoch: 962 \\| Time: 0 \\| Train Loss: 3.0153706953014767 \\| Test Loss: 1.820262105168191 \\| Test Accuracy: 0.6422936180212845\n","Epoch: 963 \\| Time: 0 \\| Train Loss: 2.9904024185421445 \\| Test Loss: 1.8627127690376641 \\| Test Accuracy: 0.5981468425823345\n","Epoch: 964 \\| Time: 0 \\| Train Loss: 2.998508228045496 \\| Test Loss: 1.8452792868593733 \\| Test Accuracy: 0.6147984019874442\n","Epoch: 965 \\| Time: 0 \\| Train Loss: 3.007877719465285 \\| Test Loss: 1.8558713786080159 \\| Test Accuracy: 0.602578305972404\n","Epoch: 966 \\| Time: 0 \\| Train Loss: 3.0068641911123626 \\| Test Loss: 1.8740071268040734 \\| Test Accuracy: 0.5852217410279652\n","Epoch: 967 \\| Time: 0 \\| Train Loss: 2.9851974915238104 \\| Test Loss: 1.8348307742581347 \\| Test Accuracy: 0.6265484943095981\n","Epoch: 968 \\| Time: 0 \\| Train Loss: 3.0415231915403345 \\| Test Loss: 1.840137589131302 \\| Test Accuracy: 0.6208748783026152\n","Epoch: 969 \\| Time: 0 \\| Train Loss: 2.979570093116017 \\| Test Loss: 1.8315831335829051 \\| Test Accuracy: 0.6299728069291973\n","Epoch: 970 \\| Time: 0 \\| Train Loss: 2.979910933236862 \\| Test Loss: 1.8253993281990673 \\| Test Accuracy: 0.6371235773995367\n","Epoch: 971 \\| Time: 0 \\| Train Loss: 3.004871204756141 \\| Test Loss: 1.8584757572591561 \\| Test Accuracy: 0.6009332930473025\n","Epoch: 972 \\| Time: 0 \\| Train Loss: 3.0010909236007466 \\| Test Loss: 1.8218888811799079 \\| Test Accuracy: 0.641017893712022\n","Epoch: 973 \\| Time: 0 \\| Train Loss: 3.037694398441233 \\| Test Loss: 1.865336554244864 \\| Test Accuracy: 0.5968039748883741\n","Epoch: 974 \\| Time: 0 \\| Train Loss: 3.0028701127463715 \\| Test Loss: 1.8587137430010947 \\| Test Accuracy: 0.6014704401248867\n","Epoch: 975 \\| Time: 0 \\| Train Loss: 2.9941891334229007 \\| Test Loss: 1.8197175279707356 \\| Test Accuracy: 0.6419579010977944\n","Epoch: 976 \\| Time: 0 \\| Train Loss: 3.0392162395159907 \\| Test Loss: 1.8583118219743981 \\| Test Accuracy: 0.6002618592003223\n","Epoch: 977 \\| Time: 0 \\| Train Loss: 2.995175428637844 \\| Test Loss: 1.833858692594864 \\| Test Accuracy: 0.6256420586161748\n","Epoch: 978 \\| Time: 0 \\| Train Loss: 3.0038869596459064 \\| Test Loss: 1.839998936960114 \\| Test Accuracy: 0.6205391613791251\n","Epoch: 979 \\| Time: 0 \\| Train Loss: 2.997368747838458 \\| Test Loss: 1.8405172011371334 \\| Test Accuracy: 0.6207405915332191\n","Epoch: 980 \\| Time: 0 \\| Train Loss: 2.9744918165740417 \\| Test Loss: 1.8317250233351416 \\| Test Accuracy: 0.6302413804679894\n","Epoch: 981 \\| Time: 0 \\| Train Loss: 2.9947294723889013 \\| Test Loss: 1.8392023891850091 \\| Test Accuracy: 0.6227884647665088\n","Epoch: 982 \\| Time: 0 \\| Train Loss: 3.025829407601387 \\| Test Loss: 1.8596759758282118 \\| Test Accuracy: 0.5984489878134757\n","Epoch: 983 \\| Time: 0 \\| Train Loss: 2.9966472580951002 \\| Test Loss: 1.8308640682646133 \\| Test Accuracy: 0.6303420955450364\n","Epoch: 984 \\| Time: 0 \\| Train Loss: 2.993311021325456 \\| Test Loss: 1.8145896344737433 \\| Test Accuracy: 0.6481686641823614\n","Epoch: 985 \\| Time: 0 \\| Train Loss: 3.0027010625393546 \\| Test Loss: 1.8232504497781843 \\| Test Accuracy: 0.6389364487863833\n","Epoch: 986 \\| Time: 0 \\| Train Loss: 2.9867449745826313 \\| Test Loss: 1.8318370549975547 \\| Test Accuracy: 0.6286635109275859\n","Epoch: 987 \\| Time: 0 \\| Train Loss: 2.9966994624617964 \\| Test Loss: 1.8144380212341766 \\| Test Accuracy: 0.6479000906435693\n","Epoch: 988 \\| Time: 0 \\| Train Loss: 3.0159831853865846 \\| Test Loss: 1.8483494243908338 \\| Test Accuracy: 0.6116426629066372\n","Epoch: 989 \\| Time: 0 \\| Train Loss: 3.0035488335218705 \\| Test Loss: 1.8213331254255107 \\| Test Accuracy: 0.6406486050961829\n","Epoch: 990 \\| Time: 0 \\| Train Loss: 3.006398335845187 \\| Test Loss: 1.8330027212912432 \\| Test Accuracy: 0.6281263638500016\n","Epoch: 991 \\| Time: 0 \\| Train Loss: 3.006783047656457 \\| Test Loss: 1.8448209783038356 \\| Test Accuracy: 0.6150669755262362\n","Epoch: 992 \\| Time: 0 \\| Train Loss: 3.005820732991732 \\| Test Loss: 1.8429570499919514 \\| Test Accuracy: 0.6160069829120086\n","Epoch: 993 \\| Time: 0 \\| Train Loss: 3.010904674409675 \\| Test Loss: 1.854327477610674 \\| Test Accuracy: 0.6069761976701246\n","Epoch: 994 \\| Time: 0 \\| Train Loss: 2.99355393486786 \\| Test Loss: 1.8158864054045452 \\| Test Accuracy: 0.6465236512572599\n","Epoch: 995 \\| Time: 0 \\| Train Loss: 3.0108135697442124 \\| Test Loss: 1.8462908963788733 \\| Test Accuracy: 0.6144291133716051\n","Epoch: 996 \\| Time: 0 \\| Train Loss: 3.0416626560825453 \\| Test Loss: 1.8485448217187317 \\| Test Accuracy: 0.6119783798301273\n","Epoch: 997 \\| Time: 0 \\| Train Loss: 2.980100071780437 \\| Test Loss: 1.8330002091984892 \\| Test Accuracy: 0.6274213583106725\n","Epoch: 998 \\| Time: 0 \\| Train Loss: 3.0010347602561493 \\| Test Loss: 1.8232333639660618 \\| Test Accuracy: 0.6371235773995367\n","Epoch: 999 \\| Time: 0 \\| Train Loss: 3.0116483656099637 \\| Test Loss: 1.8349840651254286 \\| Test Accuracy: 0.6277235035418135\n","Epoch: 1000 \\| Time: 0 \\| Train Loss: 2.984096000700596 \\| Test Loss: 1.837210431631031 \\| Test Accuracy: 0.623191325074697\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"snGD6f3JmIc0"},"execution_count":null,"outputs":[]}]}